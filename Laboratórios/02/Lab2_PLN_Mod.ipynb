{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dados:**\n",
        "\n",
        " **Aluno(a):** `Adicione seu nome aqui`\n",
        "\n",
        " **Matrícula:** `Adicione sua matrícula aqui`"
      ],
      "metadata": {
        "id": "-YPBVOwcm-xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivo:\n",
        "\n",
        "Nesse laboratório, você extrairá um romance de um arquivo HTML retirado do site Projeto Gutenberg (que contém um grande corpus de livros) usando o pacote de requisições do Python. Em seguida, você normalizará e tokenizará o texto. Você também irá analisar a distribuição de palavras. Ao final você deverá simular a execução do algoritmo BPE (Byte-Pair Encoding) e calcular a distância de edição entre duas palavras."
      ],
      "metadata": {
        "id": "AzRN0mmg27MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bibliotecas**"
      ],
      "metadata": {
        "id": "DEgDOZs7qkRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "D9tkHpFtqmTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Obtenção dos dados**\n",
        "\n",
        "Para a atividade desse laborátorio, iremos utilizar dados textuais oriundos do **[Project Gutenberg](https://www.gutenberg.org/)**. O texto escolhido é a obra Moby Dick."
      ],
      "metadata": {
        "id": "czDyUObFndhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6eYoitlP77"
      },
      "outputs": [],
      "source": [
        "#Endereço do texto\n",
        "r = requests.get(\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\")\n",
        "\n",
        "r.encoding = 'utf-8'\n",
        "\n",
        "html = r.text\n",
        "\n",
        "print(html[0:2000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir o livro\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "text = soup.get_text()\n",
        "\n",
        "print(text[32000:34000])"
      ],
      "metadata": {
        "id": "wb9wO9ufpCx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-Processamento**\n",
        "O resultado de cada uma das etapas a seguir **deve ser utilizada como entrada para as etapas posteriores!!!**"
      ],
      "metadata": {
        "id": "TQ8Uk1P_tkDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicialmente normalize o texto, tornando-o completamente minúsculo. Você pode fazer isso usando a função lower() da classe string.\n",
        "def normalizar(texto):\n",
        "    pass\n",
        "\n",
        "texto_normalizado = normalizar(text)\n",
        "print(texto_normalizado[0:50])"
      ],
      "metadata": {
        "id": "SGvDYCC4tng_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize o texto considerando apenas caracteres alpha-numéricos. Você pode fazer isso com expressões regulares e a biblioteca nltk.tokenize.RegexpTokenizer() passando seu padrão ou re.find()\n",
        "def tokenizar(texto):\n",
        "   pass\n",
        "\n",
        "\n",
        "tokens = tokenizar(texto_normalizado)\n",
        "\n",
        "print(f\"Número de tokens: {len(tokens)}\")\n",
        "print(\"Primeiros 20 tokens:\", tokens[:20])"
      ],
      "metadata": {
        "id": "rLtNJh41yQk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente o código para remover tokens que possuem tamanho maior que 15 e menor que 2. Você pode fazer isso usando a função len() para verificar o tamanho das palavras\n",
        "def remove_palavras_pequenas_grandes(tokens):\n",
        "    pass\n",
        "\n",
        "\n",
        "process_1 = remove_palavras_pequenas_grandes(tokens)\n",
        "print(f\"Número de tokens: {len(process_1)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_1[:20])"
      ],
      "metadata": {
        "id": "CBlxozG5oOsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista de tokens sem stopwords. Você pode fazer isso usando a função stopwords.words('english') para encontrar todas as stopwords\n",
        "def remove_stopwords(tokens):\n",
        "  pass\n",
        "\n",
        "\n",
        "process_2 = remove_stopwords(process_1)\n",
        "print(f\"Número de tokens: {len(process_2)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_2[:20])"
      ],
      "metadata": {
        "id": "O-b1_sHpoQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens stemmizados. Você pode fazer isso usando a função stem() da classe PorterStemmer e passando para ela a palavra desejada\n",
        "def stemming(tokens):\n",
        "  pass\n",
        "\n",
        "\n",
        "process_3 = stemming(process_2)\n",
        "print(f\"Número de tokens: {len(process_3)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_3[:20])"
      ],
      "metadata": {
        "id": "oclfKIqZplCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens lematizados. Você pode fazer isso usando a função lemmatize() da classe WordNetLemmatizer e passando para ela a palavra desejada\n",
        "def lemmatize(texto):\n",
        "  pass\n",
        "\n",
        "\n",
        "process_4 = lemmatize(process_3)\n",
        "print(f\"Número de tokens: {len(process_4)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_4[:20])"
      ],
      "metadata": {
        "id": "ac7r3rursJqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Frequência dos tokens**\n",
        "Mostre a distribuição das palavras para cada uma das etapas de pré-processamento realizadas. Você pode fazer isso utilizando a função nltk.FreqDist() passando sua lista de palavras"
      ],
      "metadata": {
        "id": "Bw7dCTUQBZJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras depois de normalizadas e das pequenas serem removidas"
      ],
      "metadata": {
        "id": "dMq3DqFP71f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas"
      ],
      "metadata": {
        "id": "4atACCWT76xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas e com stemming"
      ],
      "metadata": {
        "id": "Qf6_YOps78hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas, com stemming e lemming"
      ],
      "metadata": {
        "id": "eY8j-7P28AWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perguntas\n",
        "\n",
        "1. **Compare o texto original com o texto após a normalização. Quais diferenças você percebe nas palavras e na estrutura geral?**\n",
        "- Resposta: `aqui`\n",
        "\n",
        "2. **O que muda na quantidade de tokens ao aplicar a função remove_palavras_pequenas_grandes()? Informe abaixo o total de tokens antes e depois da aplicação da função.**\n",
        "- Resposta: `aqui`\n",
        "\n",
        "3. **Existe diferença entre o pré-processamento antes e depois de aplicar lematização e stemming?**\n",
        "- Resposta: `aqui`"
      ],
      "metadata": {
        "id": "ngWYeRR8ybtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algoritmo Byte-Pair Encoding:**"
      ],
      "metadata": {
        "id": "V71AONUTvHsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando as três palavras mais frequentes resultantes da lemmatização (process_4), execute, à mão, o algoritmo BPE para k = 3 e insira abaixo o vocabulário e regras obtidas:\n",
        "\n",
        "\n",
        "\n",
        "*   vocabulário obtido: `vocabulário aqui`\n",
        "*   regras obtidas: `regras aqui`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QEhXHfgEvOAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distância de Edição**\n",
        "Calcule à mão a distância de edição entre as palavras 'while' e 'like' supondo que inserções ou deleções tem custo 1 e uma substituição tem custo 2 quando os caracteres são diferentes e custo 0 quando são iguais.\n",
        "\n",
        "\n",
        "Insira o valor obtido aqui: `valor obtido`\n"
      ],
      "metadata": {
        "id": "OL7AUQkc_qau"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6UITqLZzKKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}