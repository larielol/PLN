{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dados:**\n",
        "\n",
        " **Aluno(a):** `Lucas Ariel Alves de Carvalho`\n",
        "\n",
        " **Matrícula:** `121210801`\n",
        ""
      ],
      "metadata": {
        "id": "-YPBVOwcm-xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivo:\n",
        "\n",
        "Nesse laboratório, você extrairá um romance de um arquivo HTML retirado do site Projeto Gutenberg (que contém um grande corpus de livros) usando o pacote de requisições do Python. Em seguida, você normalizará e tokenizará o texto. Você também irá analisar a distribuição de palavras. Ao final você deverá simular a execução do algoritmo BPE (Byte-Pair Encoding) e calcular a distância de edição entre duas palavras."
      ],
      "metadata": {
        "id": "AzRN0mmg27MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bibliotecas**"
      ],
      "metadata": {
        "id": "DEgDOZs7qkRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "D9tkHpFtqmTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad0c8e7-8d27-4566-94ad-5416579cde1f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Obtenção dos dados**\n",
        "\n",
        "Para a atividade desse laborátorio, iremos utilizar dados textuais oriundos do **[Project Gutenberg](https://www.gutenberg.org/)**. O texto escolhido é a obra Moby Dick."
      ],
      "metadata": {
        "id": "czDyUObFndhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ba6eYoitlP77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d371794b-7a33-49fe-f8b0-7cb3bea591a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\r\n",
            "<html lang=\"en\">\r\n",
            "<head>\r\n",
            "  <meta charset=\"utf-8\">\r\n",
            "<title>Moby Dick; or The Whale</title>\r\n",
            "\r\n",
            "<style>\r\n",
            "\r\n",
            "    body {margin-left:10%; margin-right:10%; text-align:justify }\r\n",
            "    p { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\r\n",
            "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\r\n",
            "    hr  { width: 50%; text-align: center;}\r\n",
            "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\r\n",
            "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\r\n",
            "    .toc       { margin-left: 10%; margin-bottom: .75em;}\r\n",
            "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\r\n",
            "\r\n",
            "    table      {margin-left: 10%;}\r\n",
            "\r\n",
            "a:link {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "link {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "a:visited {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "a:hover {color:red}\r\n",
            "\r\n",
            "</style>\r\n",
            "  </head>\r\n",
            "  <body>\r\n",
            "<div>*** START OF THE PROJECT GUTENBERG EBOOK 2701 ***</div>\r\n",
            "    <h1>\r\n",
            "      MOBY-DICK;<br><br>or, THE WHALE.\r\n",
            "    </h1>\r\n",
            "    <p>\r\n",
            "      <br>\r\n",
            "    </p>\r\n",
            "    <h2>\r\n",
            "      By Herman Melville\r\n",
            "    </h2>\r\n",
            "    <p>\r\n",
            "      <br> <br>\r\n",
            "    </p>\r\n",
            "    <hr>\r\n",
            "    <p>\r\n",
            "      <br> <br>\r\n",
            "    </p>\r\n",
            "    <blockquote>\r\n",
            "      <p class=\"toc\" style=\"font-size: x-large;\">\r\n",
            "        <b>CONTENTS</b>\r\n",
            "      </p>\r\n",
            "      <p>\r\n",
            "        <br>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2H_4_0002\"> ETYMOLOGY. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2H_4_0003\"> EXTRACTS (Supplied by a Sub-Sub-Librarian).\r\n",
            "        </a>\r\n",
            "      </p>\r\n",
            "      <p>\r\n",
            "        <br>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2HCH0001\"> CHAPTER 1. Loomings. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2HCH0002\"> CHAPTER 2. The Carpet-Bag. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2HCH0003\"> CHAPTER 3. The Spouter-Inn. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#lin\n"
          ]
        }
      ],
      "source": [
        "#Endereço do texto\n",
        "r = requests.get(\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\")\n",
        "\n",
        "r.encoding = 'utf-8'\n",
        "\n",
        "html = r.text\n",
        "\n",
        "print(html[0:2000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir o livro\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "text = soup.get_text()\n",
        "\n",
        "print(text[32000:34000])"
      ],
      "metadata": {
        "id": "wb9wO9ufpCx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82000bae-7bc8-4efd-fa8a-bc067f5f1a86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h her surf.\r\n",
            "      Right and left, the streets take you waterward. Its extreme downtown is\r\n",
            "      the battery, where that noble mole is washed by waves, and cooled by\r\n",
            "      breezes, which a few hours previous were out of sight of land. Look at the\r\n",
            "      crowds of water-gazers there.\r\n",
            "    \n",
            "\r\n",
            "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\r\n",
            "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\r\n",
            "      you see?—Posted like silent sentinels all around the town, stand\r\n",
            "      thousands upon thousands of mortal men fixed in ocean reveries. Some\r\n",
            "      leaning against the spiles; some seated upon the pier-heads; some looking\r\n",
            "      over the bulwarks of ships from China; some high aloft in the rigging, as\r\n",
            "      if striving to get a still better seaward peep. But these are all\r\n",
            "      landsmen; of week days pent up in lath and plaster—tied to counters,\r\n",
            "      nailed to benches, clinched to desks. How then is this? Are the green\r\n",
            "      fields gone? What do they here?\r\n",
            "    \n",
            "\r\n",
            "      But look! here come more crowds, pacing straight for the water, and\r\n",
            "      seemingly bound for a dive. Strange! Nothing will content them but the\r\n",
            "      extremest limit of the land; loitering under the shady lee of yonder\r\n",
            "      warehouses will not suffice. No. They must get just as nigh the water as\r\n",
            "      they possibly can without falling in. And there they stand—miles of\r\n",
            "      them—leagues. Inlanders all, they come from lanes and alleys,\r\n",
            "      streets and avenues—north, east, south, and west. Yet here they all\r\n",
            "      unite. Tell me, does the magnetic virtue of the needles of the compasses\r\n",
            "      of all those ships attract them thither?\r\n",
            "    \n",
            "\r\n",
            "      Once more. Say you are in the country; in some high land of lakes. Take\r\n",
            "      almost any path you please, and ten to one it carries you down in a dale,\r\n",
            "      and leaves you there by a pool in the stream. There is magic in it. Let\r\n",
            "      the most absent-minded of men be plunged in his deepest r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-Processamento**\n",
        "O resultado de cada uma das etapas a seguir **deve ser utilizada como entrada para as etapas posteriores!!!**"
      ],
      "metadata": {
        "id": "TQ8Uk1P_tkDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicialmente normalize o texto, tornando-o completamente minúsculo. Você pode fazer isso usando a função lower() da classe string.\n",
        "def normalizar(texto):\n",
        "    return texto.lower()\n",
        "\n",
        "texto_normalizado = normalizar(text)\n",
        "print(texto_normalizado[0:50])"
      ],
      "metadata": {
        "id": "SGvDYCC4tng_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b906b1-2aa9-4b73-abea-2f321d7c0861"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "moby dick; or the whale\n",
            "\n",
            "\n",
            "\n",
            "*** start of the pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize o texto considerando apenas caracteres alpha-numéricos. Você pode fazer isso com expressões regulares e a biblioteca nltk.tokenize.RegexpTokenizer() passando seu padrão ou re.find()\n",
        "def tokenizar(texto):\n",
        "   tokenizer = RegexpTokenizer(r'\\w+')\n",
        "   return tokenizer.tokenize(texto)\n",
        "\n",
        "\n",
        "tokens = tokenizar(texto_normalizado)\n",
        "\n",
        "print(f\"Número de tokens: {len(tokens)}\")\n",
        "print(\"Primeiros 20 tokens:\", tokens[:20])"
      ],
      "metadata": {
        "id": "rLtNJh41yQk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db77e9e-f8d3-4055-8259-6cf1f4dbc954"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 219426\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'or', 'the', 'whale', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente o código para remover tokens que possuem tamanho maior que 15 e menor que 2. Você pode fazer isso usando a função len() para verificar o tamanho das palavras\n",
        "def remove_palavras_pequenas_grandes(tokens):\n",
        "    return [token for token in tokens if 2 <= len(token) <= 15]\n",
        "\n",
        "\n",
        "process_1 = remove_palavras_pequenas_grandes(tokens)\n",
        "print(f\"Número de tokens: {len(process_1)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_1[:20])"
      ],
      "metadata": {
        "id": "CBlxozG5oOsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530d9ff7-93ac-4349-f09b-3e60773a5639"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 210173\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'or', 'the', 'whale', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista de tokens sem stopwords. Você pode fazer isso usando a função stopwords.words('english') para encontrar todas as stopwords\n",
        "def remove_stopwords(tokens):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "\n",
        "process_2 = remove_stopwords(process_1)\n",
        "print(f\"Número de tokens: {len(process_2)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_2[:20])"
      ],
      "metadata": {
        "id": "O-b1_sHpoQP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834df9b1-6494-43a3-e208-0c2f63dcad3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 111191\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'whale', 'herman', 'melville', 'contents', 'etymology', 'extracts', 'supplied', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens stemmizados. Você pode fazer isso usando a função stem() da classe PorterStemmer e passando para ela a palavra desejada\n",
        "def stemming(tokens):\n",
        "  stemmer = PorterStemmer()\n",
        "  return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "\n",
        "process_3 = stemming(process_2)\n",
        "print(f\"Número de tokens: {len(process_3)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_3[:20])"
      ],
      "metadata": {
        "id": "oclfKIqZplCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63435fbf-b4d6-44da-e1ac-f41647cf6a00"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 111191\n",
            "Primeiros 20 tokens: ['mobi', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'mobi', 'dick', 'whale', 'herman', 'melvil', 'content', 'etymolog', 'extract', 'suppli', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens lematizados. Você pode fazer isso usando a função lemmatize() da classe WordNetLemmatizer e passando para ela a palavra desejada\n",
        "def lemmatize(texto):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return [lemmatizer.lemmatize(token) for token in texto]\n",
        "\n",
        "\n",
        "process_4 = lemmatize(process_3)\n",
        "print(f\"Número de tokens: {len(process_4)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_4[:20])"
      ],
      "metadata": {
        "id": "ac7r3rursJqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96778b59-a195-4941-c3a0-20970690f720"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 111191\n",
            "Primeiros 20 tokens: ['mobi', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'mobi', 'dick', 'whale', 'herman', 'melvil', 'content', 'etymolog', 'extract', 'suppli', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Frequência dos tokens**\n",
        "Mostre a distribuição das palavras para cada uma das etapas de pré-processamento realizadas. Você pode fazer isso utilizando a função nltk.FreqDist() passando sua lista de palavras"
      ],
      "metadata": {
        "id": "Bw7dCTUQBZJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras depois de normalizadas e das pequenas serem removidas\n",
        "from nltk import FreqDist\n",
        "frequencia = FreqDist(tokens)\n",
        "frequencia.most_common(20)\n"
      ],
      "metadata": {
        "id": "dMq3DqFP71f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570aa397-2e10-47c6-dd2f-71391453b472"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 14538),\n",
              " ('of', 6626),\n",
              " ('and', 6447),\n",
              " ('a', 4747),\n",
              " ('to', 4627),\n",
              " ('in', 4184),\n",
              " ('that', 3085),\n",
              " ('his', 2532),\n",
              " ('it', 2522),\n",
              " ('i', 2127),\n",
              " ('he', 1897),\n",
              " ('but', 1818),\n",
              " ('s', 1812),\n",
              " ('as', 1742),\n",
              " ('is', 1725),\n",
              " ('with', 1723),\n",
              " ('was', 1645),\n",
              " ('for', 1618),\n",
              " ('all', 1527),\n",
              " ('this', 1396)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas\n",
        "from nltk import FreqDist\n",
        "frequencia2 = FreqDist(process_2)\n",
        "frequencia2.most_common(20)\n"
      ],
      "metadata": {
        "id": "4atACCWT76xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cec2b2f-9f4e-4f9e-e61b-e871a4ffc6e7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('whale', 1240),\n",
              " ('one', 923),\n",
              " ('like', 647),\n",
              " ('upon', 566),\n",
              " ('man', 527),\n",
              " ('ship', 519),\n",
              " ('ahab', 517),\n",
              " ('ye', 473),\n",
              " ('sea', 455),\n",
              " ('old', 450),\n",
              " ('would', 432),\n",
              " ('though', 384),\n",
              " ('head', 348),\n",
              " ('yet', 345),\n",
              " ('boat', 337),\n",
              " ('time', 334),\n",
              " ('long', 333),\n",
              " ('captain', 329),\n",
              " ('still', 312),\n",
              " ('chapter', 308)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas e com stemming\n",
        "from nltk import FreqDist\n",
        "frequencia3 = FreqDist(process_3)\n",
        "frequencia3.most_common(20)"
      ],
      "metadata": {
        "id": "Qf6_YOps78hI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79262aae-8129-43be-87d2-a67249ba5c50"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('whale', 1646),\n",
              " ('one', 943),\n",
              " ('like', 661),\n",
              " ('ship', 625),\n",
              " ('upon', 566),\n",
              " ('ye', 548),\n",
              " ('sea', 542),\n",
              " ('man', 541),\n",
              " ('ahab', 518),\n",
              " ('boat', 484),\n",
              " ('head', 469),\n",
              " ('seem', 463),\n",
              " ('old', 450),\n",
              " ('time', 449),\n",
              " ('would', 432),\n",
              " ('though', 384),\n",
              " ('hand', 368),\n",
              " ('look', 366),\n",
              " ('captain', 353),\n",
              " ('yet', 345)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas, com stemming e lemming\n",
        "from nltk import FreqDist\n",
        "frequencia4 = FreqDist(process_4)\n",
        "frequencia4.most_common(20)"
      ],
      "metadata": {
        "id": "eY8j-7P28AWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2e3384-297e-4df4-bf00-311fe67aa724"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('whale', 1646),\n",
              " ('one', 943),\n",
              " ('like', 661),\n",
              " ('ship', 625),\n",
              " ('upon', 566),\n",
              " ('ye', 548),\n",
              " ('sea', 542),\n",
              " ('man', 541),\n",
              " ('ahab', 518),\n",
              " ('boat', 484),\n",
              " ('head', 469),\n",
              " ('seem', 463),\n",
              " ('old', 450),\n",
              " ('time', 449),\n",
              " ('would', 432),\n",
              " ('though', 384),\n",
              " ('hand', 368),\n",
              " ('look', 366),\n",
              " ('captain', 353),\n",
              " ('yet', 345)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perguntas\n",
        "\n",
        "1. **Compare o texto original com o texto após a normalização. Quais diferenças você percebe nas palavras e na estrutura geral?**\n",
        "- Resposta: `A normalização converte todo o texto para minúsculas, eliminando diferenças entre maiúsculas e minúsculas que podem representar a mesma palavra. Isso uniformiza o texto mas remove nomes próprios que iniciam com letra maiúscula.`\n",
        "\n",
        "2. **O que muda na quantidade de tokens ao aplicar a função remove_palavras_pequenas_grandes()? Informe abaixo o total de tokens antes e depois da aplicação da função.**\n",
        "- Resposta: `Antes: 219426 / Depois: 210173. A função remove palavras muito curtas (como artigos e preposições de uma letra) e muito longas (que podem ser erros ou palavras pouco comuns), tem que estar >=2 e <= 15.`\n",
        "\n",
        "3. **Existe diferença entre o pré-processamento antes e depois de aplicar lematização e stemming?**\n",
        "- Resposta: `Sim, o stemming reduz palavras para sua raiz, podendo criar formas não existentes no dicionário, enquanto a lematização transforma palavras para sua forma no dicionario, mantendo palavras válidas.`"
      ],
      "metadata": {
        "id": "ngWYeRR8ybtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algoritmo Byte-Pair Encoding:**"
      ],
      "metadata": {
        "id": "V71AONUTvHsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando as três palavras mais frequentes resultantes da lemmatização (process_4), execute, à mão, o algoritmo BPE para k = 3 e insira abaixo o vocabulário e regras obtidas:\n",
        "\n",
        "Palavras mais frequentes: \"whale\" (1646), \"one\" (943), \"like\" (661)\n",
        "\n",
        "*   vocabulário obtido: `'w', 'h', 'a', 'l', 'e', 'o', 'n', 'i', 'k', 'wh', 'le', 'ale'`\n",
        "*   regras obtidas: `\n",
        "                      + e → le (2307 ocorrências)\n",
        "                      w + h → wh (1646 ocorrências)\n",
        "                      a + le → ale (1646 ocorrências)`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QEhXHfgEvOAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distância de Edição**\n",
        "Calcule à mão a distância de edição entre as palavras 'while' e 'like' supondo que inserções ou deleções tem custo 1 e uma substituição tem custo 2 quando os caracteres são diferentes e custo 0 quando são iguais.\n",
        "\n",
        "\n",
        "Insira o valor obtido aqui: `Total: 1 + 2 + 0 + 2 + 0 = 5`\n",
        "\n",
        "Motivo: Remover 'w' (Custo 1) + Substituir 'h' por 'l' (custo 2) + Manter 'i' (custo 0) + Substituir 'l' por 'k' (custo 2) + Manter 'e' (custo 0)"
      ],
      "metadata": {
        "id": "OL7AUQkc_qau"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6UITqLZzKKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}