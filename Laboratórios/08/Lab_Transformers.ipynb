{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wRvo9jIop0IP",
        "78wMBytIp3fn",
        "2PSPoZyVp3cc",
        "mNT9nR2ep3X1",
        "5nk5hgyTr68E",
        "W2GWG2Ear644",
        "VZamy1BYum_V",
        "2P83maF9sYZP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Identificação do aluno**\n",
        "\n",
        "**Email:**\n",
        "\n",
        "**Matrícula:**"
      ],
      "metadata": {
        "id": "BuWu3uqDpr1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratório: DistilBERT com Máscara de Linguagem (IMDB)\n",
        "\n",
        "Diferente dos laboratórios anteriores, onde a maioria das seções já estava pronta, **neste laboratório você será o protagonista da construção**.  \n",
        "Você deverá se basear no exemplo visto em aula e **desenvolver sua própria versão**, colocando sua **personalidade** e aplicando **toques extras** para buscar um melhor desempenho.  \n",
        "\n",
        "Relembrando o laboratório exemplificado em aula:  \n",
        "- Ele utiliza o modelo **DistilBERT** em uma tarefa de **Masked Language Modeling (MLM)**.  \n",
        "- O objetivo é treinar o modelo em um conjunto de dados (IMDB) e observar sua capacidade de prever palavras mascaradas em frases.  \n",
        "- O processo envolve:  \n",
        "  1. Preparação dos dados.  \n",
        "  2. Tokenização.  \n",
        "  3. Criação de máscaras nas frases.  \n",
        "  4. Fine-tuning do modelo.  \n",
        "  5. Avaliação dos resultados.  \n",
        "\n",
        "Agora é a sua vez!  \n",
        "Use o que foi mostrado em aula como **guia**, mas explore variações. Use essas **DICAS**:  \n",
        "- Experimente **diferentes taxas de aprendizado**.  \n",
        "- Aplique **diferentes tamanhos de batch**.  \n",
        "- Teste **número de épocas distintos**.\n",
        "- Use **diferentes taxas de mascaramento**.   \n",
        "- Pense em **formas criativas de avaliar o desempenho**.  \n",
        "\n",
        "O objetivo não é apenas replicar o exemplo, mas **melhorá-lo com suas próprias ideias**."
      ],
      "metadata": {
        "id": "UL9t2eW2pvVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas"
      ],
      "metadata": {
        "id": "wRvo9jIop0IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM # DICA: Tente usar outras versões de BERT"
      ],
      "metadata": {
        "id": "0VqT3Hv4qKPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conjunto de Dados"
      ],
      "metadata": {
        "id": "78wMBytIp3fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esse conjunto de dados é relacionado ao IMDB (Plataforma que contempla resenhas sobre filmes)\n",
        "dataset = load_dataset('imdb')\n",
        "print(dataset['train'][0]['text'])"
      ],
      "metadata": {
        "id": "BONhYnnxqU5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def stratified_subset_dataset(dataset_dict, n_samples_per_split, seed=42):\n",
        "    \"\"\"\n",
        "    Recebe um DatasetDict (por exemplo, {'train': ..., 'test': ...})\n",
        "    e retorna outro DatasetDict com a mesma estrutura,\n",
        "    mas com no máximo n_samples_per_split exemplos em cada split,\n",
        "    mantendo a proporção das labels.\n",
        "    \"\"\"\n",
        "    new_splits = {}\n",
        "\n",
        "    for split_name, split_data in dataset_dict.items():\n",
        "        labels = np.array(split_data['label'])\n",
        "        indices = np.arange(len(labels))\n",
        "        # Seleção estratificada\n",
        "        selected_indices, _ = train_test_split(\n",
        "            indices,\n",
        "            train_size=min(n_samples_per_split, len(labels)),\n",
        "            stratify=labels,\n",
        "            random_state=seed\n",
        "        )\n",
        "        new_splits[split_name] = split_data.select(selected_indices)\n",
        "\n",
        "    return DatasetDict(new_splits)\n",
        "\n",
        "\n",
        "subset_dataset = stratified_subset_dataset(dataset, n_samples_per_split=2000)  # DICA: tente modificar o parâmetro n_samples_per_split"
      ],
      "metadata": {
        "id": "YN6705jHqkeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(subset_dataset['train']))\n",
        "print(len(subset_dataset['test']))"
      ],
      "metadata": {
        "id": "rU_yJrylqkXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   ## **Para as próximas etapas use o laborátorio apresentado em aula**"
      ],
      "metadata": {
        "id": "5BuHAwporNLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenização\n",
        "\n",
        "Nessa seção vocês vão aprender a **transformar frases em tokens** (unidades menores de texto, como palavras ou subpalavras), que é o formato que o modelo consegue entender.  \n",
        "\n",
        "Lembre-se:  \n",
        "- O modelo **DistilBERT** não trabalha diretamente com texto cru, mas sim com **IDs numéricos** que representam tokens.  \n",
        "- O **tokenizer** faz esse processo automaticamente:  \n",
        "  1. Divide a frase em tokens.  \n",
        "  2. Converte os tokens para IDs.  \n",
        "  3. Garante que todos os exemplos tenham o mesmo tamanho (com *padding* e *truncation*). Lembre-se de tentar diferentes tamanhos de tokens de entrada do modelo.  "
      ],
      "metadata": {
        "id": "2PSPoZyVp3cc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TuvS4kpsHSMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adicionando Mascara aos Dados\n",
        "\n",
        "Nessa etapa vocês vão aplicar a **máscara de linguagem (MLM)**, que é o coração do pré-treinamento do BERT/DistilBERT.  \n",
        "A ideia é **esconder (mascarar) aleatoriamente tokens** da frase e pedir para o modelo tentar prever quais palavras estavam lá.  \n",
        "\n",
        "**Dica:**\n",
        "- Não é preciso reinventar tudo: a biblioteca `transformers` já possui funções que ajudam a criar essas máscaras.  \n",
        "- Testem mascarar as frases considerando outras proporções além dos 15% originais do artigo.   "
      ],
      "metadata": {
        "id": "mNT9nR2ep3X1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4l5qkNY6HTsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adequando os Dados\n",
        "\n",
        "Para treinar um modelo de linguagem de forma eficiente, precisamos **organizar os dados em lotes (batches)** e colocá-los em uma estrutura que o modelo consiga consumir durante o treinamento.  \n",
        "\n",
        "**Dica:**\n",
        "- Testem com diferentes tamanhos de `batch_size` e observem como isso afeta a memória e a velocidade.  \n",
        "- Conferir os **shapes** dos tensores é sempre uma boa prática para evitar erros no treinamento.  "
      ],
      "metadata": {
        "id": "5nk5hgyTr68E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAb8E1KDH0uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando dados de um modelo pré-treinado para continuar o pré-treinamento\n",
        "\n",
        "Até aqui vocês aprenderam a preparar dados (tokenizar, mascarar e estruturar em batches).  \n",
        "Agora, em vez de treinar um modelo do zero, vamos **aproveitar o conhecimento de um modelo já pré-treinado** e continuar o pré-treinamento com o nosso conjunto de dados (IMDB).  \n",
        "\n",
        "Por que usar um modelo pré-treinado?\n",
        "- Modelos como **DistilBERT** já foram treinados em enormes quantidades de texto.  \n",
        "- Isso significa que eles **já entendem bastante da linguagem** (sintaxe, gramática, vocabulário, contexto).  \n",
        "- Ao continuar o pré-treinamento com um novo corpus, você adapta o modelo para **captar melhor o estilo e o domínio** dos seus dados.  \n",
        "- Lembre-se de experimentar diferentes quantidades de épocas e valores de learning-rate.  \n"
      ],
      "metadata": {
        "id": "W2GWG2Ear644"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dcQ6jn6WIKRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testando as Predições"
      ],
      "metadata": {
        "id": "VZamy1BYum_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Atenção use esses três exemplos para testar!!!**"
      ],
      "metadata": {
        "id": "BWCmaskgytAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 1\n",
        "text = \"The movie was really [MASK].\"\n",
        "\n",
        "# Exemplo 2\n",
        "text = \"The food at the restaurant was absolutely [MASK].\"\n",
        "\n",
        "# Exemplo 3\n",
        "text = \"The weather today is very [MASK].\""
      ],
      "metadata": {
        "id": "qDo7NhFeyrVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busque alguma frase de exemplo da base de dados de teste e coloque a máscara em lugares variados do texto."
      ],
      "metadata": {
        "id": "n1AoTRfjL7Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Busque alguma frase no conjunto de teste\n",
        "text = \"... [MASK] ...\""
      ],
      "metadata": {
        "id": "BSe_ipPSL_q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Downstream (Classificação)\n",
        "\n",
        "Até agora o foco foi no **pré-treinamento com Masked Language Modeling (MLM)**, onde o objetivo era prever palavras mascaradas.  \n",
        "Mas o poder do BERT/DistilBERT aparece de verdade quando usamos esse conhecimento adquirido em **tarefas downstream** (tarefas específicas), como classificação.  \n",
        "\n",
        "**O que muda aqui?**\n",
        "- No pré-treinamento, o modelo aprende sobre a **linguagem em geral**.  \n",
        "- No fine-tuning, ajustamos o modelo para uma tarefa **específica**, por exemplo:  \n",
        "  - **Classificação de sentimentos** (positivo/negativo em reviews do IMDB).  \n",
        "  - **Classificação de tópicos**.  \n",
        "  - **Detecção de spam**.  \n",
        "\n",
        "**O que vocês precisam fazer aqui:**\n",
        "- Utilizar um modelo pré-treinado (pode ser o seu ou não) (`DistilBERT`).  \n",
        "- Adaptá-lo para classificação usando `TFDistilBertForSequenceClassification` (ou versão PyTorch).  \n",
        "- Preparar os dados de entrada (frases e rótulos → `0` para negativo, `1` para positivo, por exemplo).  \n",
        "- Treinar o modelo nos dados rotulados.\n",
        "  \n",
        "**Objetivo final:**  \n",
        "Transformar um modelo genérico (pré-treinado em linguagem) em um modelo **especializado em classificar sentimentos no IMDB**.\n",
        "\n",
        "**Dicas:**\n",
        "- Ajustem **taxa de aprendizado** e **batch size**: esses hiperparâmetros têm forte impacto no desempenho.  \n",
        "- Depois de treinar, calcule as métricas para verificar se o modelo capturou bem os padrões de classificação.\n",
        "- Tente obter resultados melhores do que os do notebook exemplo da aula.  \n",
        "\n"
      ],
      "metadata": {
        "id": "2P83maF9sYZP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWN6aHVAIsLf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}