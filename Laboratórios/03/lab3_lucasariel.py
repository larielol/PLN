# -*- coding: utf-8 -*-
"""Lab3_LucasAriel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOIjyhIEGHDdIFimjuSRyMLEbJiKEroS

# **Dados:**

 **Aluno(a):** `Lucas Ariel Alves de Carvalho`

 **Matrícula:** `121210801`

## Objetivo do Laboratório

Neste laboratório, você irá **analisar a lei de Zipf e a construção de modelos de linguagem N-grams** em um texto. O objetivo é verificar se a lei se mantém em diversas etapas do pré-processamento. Além disso, você irá gerar seu próprio modelo de linguagem n-grama.

### Bibliotecas que você pode utilizar:

- [NLTK (Natural Language Toolkit)](https://www.nltk.org/): Uma das bibliotecas mais tradicionais, com ferramentas para tokenização, stemming, stopwords, N-grams, entre outras.
- [spaCy](https://spacy.io/): Biblioteca moderna e eficiente para tarefas de PLN, como lematização, dependência sintática e análise gramatical.
- [Scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction): Possui recursos para extração de características a partir de texto (como vetorização e contagem de N-grams).

**Dica:** Comece com o **NLTK**, pois ela tem suporte direto e simples para geração de **N-grams**.

## **Bibliotecas**
"""

import requests
from bs4 import BeautifulSoup
import nltk
import re

# Import StopWords
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import PorterStemmer

# Importação de Modelos Bi-Grams e Tri-Grams
from nltk import bigrams, trigrams

"""## **Obtenção dos Dados**

Para a atividade desse laborátorio, iremos utilizar dados textuais oriundos do **[Project Gutenberg](https://www.gutenberg.org/)**. O texto escolhido é a obra Dom Casmurro.
"""

#Endereço do texto e seu download
r = requests.get("https://www.gutenberg.org/cache/epub/55752/pg55752-images.html")

r.encoding = 'utf-8'

html = r.text

print(html[0:2000])

# Exibir o livro
soup = BeautifulSoup(html, 'html.parser')

texto = soup.get_text()

print(texto[32000:34000])

#Remove informações do início e fim do capítulo. Elas não são importantes
def limpar_texto_gutenberg(texto):
    inicio_frase = "Do titulo."
    inicio = texto.find(inicio_frase)
    fim = texto.find("*** END OF THE PROJECT")
    if inicio != -1 and fim != -1:
        return texto[inicio + len(inicio_frase):fim]
    return texto

texto_original = limpar_texto_gutenberg(texto)
print(texto_original[:700])

"""# Lei de Zipf

Vamos analisar se o texto obedece a lei de Zipf. Devemos investigar se ocorre o fenômeno da cauda longa, ou seja, poucas palavras com alta frequência, e muitas palavras com baixa frequência. Faça o que se pede a seguir:

1. Observe se o texto original segue a lei de zipf. Para isso, tokenize o texto considerando somente palavras alfanuméricas. Depois calcule sua frequência e exiba um gráfico da contagem por palavra.
"""

#1. Você pode realizar a tokenização utilizando a biblioteca re.find(), re.split() ou a nltk.tokenize.RegexpTokenizer() passando sua expressão e chamando o método tokenize
# Tokenize e armazene o resultado em uma variável chamada tokens
tokenizer = nltk.tokenize.RegexpTokenizer('[^\s,\d]+')
tokens = tokenizer.tokenize(texto_original)

print(tokens[0:8])

# Commented out IPython magic to ensure Python compatibility.
#Você pode contar a frequência de cada palavra utilizando a biblioteca nltk.FreqDist(lista de palavras)
#Você pode plotar o gráfico com o método plot() do objeto nltk.FreqDist() exemplo a seguir
# %matplotlib inline
contagem = nltk.FreqDist(tokens)


#Seu código de contagem aqui, armazenando na variável contagem
contagem.plot(50)

"""2. Agora normalize o texto colocando todas as palavras da lista de tokens da etapa anterior em caixa baixa e verifique como se comporta a nova distribuição no gráfico."""

#Você colocar cada palavra em caixa baixa percorrendo os tokens originais e utilizando a função lower() da própria classe String.
#Cada novo token que foi colocado em caixa baixa pode ser adicionado a uma lista auxiliar tokens_lw
tokens_lw = [token.lower() for token in tokens]


print(tokens_lw[0:5])

# Commented out IPython magic to ensure Python compatibility.
#Refaça a contagem e plote o novo gráfico utilizando a mesma biblioteca da etapa anterior
# %matplotlib inline
contagem_lower = nltk.FreqDist(tokens_lw)


#Seu código de contagem aqui, armazenando na variável contagem_lower
contagem_lower.plot(50)

"""3. Remova as stopwords da lista de tokens da etapa anterior e refaça o processo para verificar se a lei de Zipf se mantém. Para isso, utilize stopwords da mesma língua do texto."""

#Você pode remover as stopwords recuperando-as da lista da biblioteca stopwords.words('portuguese')
#A verificação pode ser feita, percorrendo sua lista de tokens atual e verificando se aquele token não é uma stopword
#Os tokens que não são stopwords devem ser adicionados a uma lista tokens_lw_sw
stop_words = set(stopwords.words('portuguese'))
tokens_lw_sw = [token for token in tokens_lw if token not in stop_words]


print(tokens_lw_sw[0:8])

# Commented out IPython magic to ensure Python compatibility.
#Refaça a contagem com nltk.FreqDist e plote o gráfico
# %matplotlib inline
contagem_lw_ns = nltk.FreqDist(tokens_lw_sw)


#Seu código de contagem aqui
contagem_lw_ns.plot(50)

"""4. Agora aplique a etapa de stemming na lista de tokens da etapa e refaça o processo para verificar se a lei de Zipf se mantém. Para isso utilize alguma biblioteca de stemming."""

#Você pode aplicar a etapa de stemming utilizando a função stem(token) da classe PorterStemmer() em cada token da lista
#Cada token que passou pelo processo de stemming deve ser adicionado à lista tokens_lw_sw_st
stemmer = PorterStemmer()
tokens_lw_sw_st = [stemmer.stem(token) for token in tokens_lw_sw]


print(tokens_lw_sw_st[0:5])

# Commented out IPython magic to ensure Python compatibility.
#Refaça a contagem e plote o novo gráfico utilizando a mesma biblioteca anterior
# %matplotlib inline
contagem_lw_sw_st = nltk.FreqDist(tokens_lw_sw_st)


#Seu código de contagem aqui
contagem_lw_sw_st.plot(50)

"""5. Ao invés de considerar apenas a ocorrências das palavras unitárias (unigramas), agora utilize bigramas e trigramas. Plote o gráfico e verifique se a lei se mantém para essas novas estruturas. Faça isso com o texto original."""

#Faça a contagem para o texto original que foi apenas tokenizado sem nenhum pré-processamento.
#Para contar os bigramas e trigramas você pode utilizar as funções bigrams e trigrams do nltk, o resultado deve ser transformado em lista
bigram_tokens = list(bigrams(tokens))
trigram_tokens = list(trigrams(tokens))



print(bigram_tokens[0:5])
print(trigram_tokens[0:5])

# Commented out IPython magic to ensure Python compatibility.
#Faça a contagem dos bigramas e trigramas e plote cada um em um gráfico, você pode aplicar a nltk.FreqDist(bigramas ou trigramas)
# %matplotlib inline
contagem_bigram_tokens = nltk.FreqDist(bigram_tokens)


#Seu código de contagem aqui
contagem_bigram_tokens.plot(50)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
contagem_trigram_tokens = nltk.FreqDist(trigram_tokens)


#Seu código de contagem aqui
contagem_trigram_tokens.plot(50)

"""## Perguntas (Lei de Zipf)

**1. Considerando o texto sem nenhum tipo de pré-processamento (apenas tokenização), o livro segue a Lei de Zipf? Também considerando bigramas e trigramas? Justifique.**
- Resposta: `Sim, o livro segue a Lei de Zipf tanto para palavras individuais (unigramas) quanto para bigramas e trigramas. Nos gráficos podemos observar essa distribuição característica, onde uma pequena quantidade de palavras/sequências aparece muitas vezes, enquanto a grande maioria aparece poucas vezes.`

**2. Considerando as etapas do pré-processamento, a lei de Zipf se mantém entre elas? Justifique.**
- Resposta: `Sim, a Lei de Zipf se mantém em todas as etapas de pré-processamento. A relação descrita por Zipf permanece - poucos itens com alta frequência e muitos com baixa frequência.`

**3. Em alguma das etapas do pré-processmaneto, as palavras mais frequentes são relacionadas com o enredo do livro?**
- Resposta: `Sim, nessa etapa steming, as palavras mais frequentes tendem a ser substantivos e verbos que realmente refletem o conteúdo e temas do livro, como nomes de personagens e ações`

## **Modelos de Linguagem N-Gramas**
Vamos gerar modelos de linguagem utilizando n-gramas e verificar se é possível predizer textos que se assemelham ao texto original. Nessa etapa, não precisaremos utilizar todas as tarefas do pré-processamento, pois pretendemos que nosso modelo seja o mais fiel possível aos dados originais. Siga os passos abaixo:

## **Modelos Bi-Gram**

1. Antes de tokenizarmos, é necessário fazermos alguns ajustes no texto original, para que nosso modelo possa capturar a distribuição real das sequências. Para isso, em cada sentença, devemos incluir o marcardor `<s>` no início e `</s>` no fim.
"""

#Você pode adicionar os marcadores de início e fim do texto utilizando regex.
#Inicialmente vamos substituir a sequência ... pelo caractere vazio, pois há várias ocorrências de ...
#Dica: use a função re.sub()
texto_mk = '<s> ' + texto_original
texto_mk = re.sub('\.\.\.', '', texto_mk)
texto_mk = texto_mk + ' </s>'


print(texto_mk[:200])

#Sempre que aparece um ponto final, na maioria das vezes isso significa o fim de uma sentença e o início de uma nova sentença.
#podemos trocar todos os pontos que são de borda (seguidos ou não por espaço) por </s> <s> .
#Dica: use a função re.sub()
texto_mk = re.sub(r'\.\s*\.\s*\.', ' </s> <s> ', texto_mk)


print(texto_mk[:200])

"""2. Agora devemos tokenizar considerando somente palavras que contém letras mantendo os marcadores de início e fim para mantermos a distribuição original das palavras nas sentenças do nosso texto."""

# A Tokenização pode ser feita com a biblioteca re. ou nltk.tokenize.RegexpTokenizer() chamando o método tokenize(lista de tokens).
# Essa parte está completa! Esse é um exemplo de função que você pode usar para fazer a tokenização:
tokenizer = nltk.tokenize.RegexpTokenizer('[^\s,\d]+')
tokens_mk = tokenizer.tokenize(texto_mk)


print(tokens_mk[0:40])

# Transforme cada token da etapa original para caixa baixa. Isso possibilita ao modelo uma maior generalização.
# Para isso, você pode percorrer sua lista de tokens e aplicar a função lower() em cada token, salvando o token na lista tokens_mk_lw
tokens_mk_lw = [token.lower() for token in tokens_mk]



print(tokens_mk_lw[0:10])

"""3. Vamos gerar os bigramas utilizando a lista de tokens da etapa anterior assim como sua contagem."""

#Você pode utilizar a função bigrams na sua lista de tokens e salvar o resultado na variável bigrams_mk
bigrams_mk = list(bigrams(tokens_mk_lw))



print(bigrams_mk[:10])

"""4. Vamos remover o bigrama (`<\s>`, `<s>`), pois eles não fazem parte da distribuição real nas sentenças."""

#Essa remoção evita que o bigrama (<\s>, <s>) influencie nas contagens
#Essa parte está completa!
bigrams_mk_filtrado = [bg for bg in bigrams_mk if bg != ('</s>', '<s>')]
print(bigrams_mk_filtrado[:10])

"""### **Probabilidade Condicional - Bi-Gram**
Nessa etapa, vamos calcular a probabilidade condicional de cada bigrama. Lembrando que isso pode ser feito pelo estimador de máxima verossimilhança:


![bigram_MLE.png](data:image/png;base64,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)

1. Pela fórmula , devemos contar a frequência dos bigramas (Numerador) assim como a frequência de de cada palavra anterior (Denominador).
"""

#Para contar unigramas você pode utilizar a nltk.FreqDist() passando a lista de tokens
freq_unigram = nltk.FreqDist(tokens_mk_lw)
freq_bigram = nltk.FreqDist(bigrams_mk_filtrado)


print(freq_unigram.most_common(5))
print(freq_bigram.most_common(5))

"""2. Calcule cada probabilidade condicional. Para cada bigrama, divida sua contagem pela contagem da primeira palavra do bigrama."""

#A estrutura abaixo já percorre cada bigrama sendo possível acessar suas palavras w1 e w2.
#Você pode usar essas palavras para acessar a frequência do bigrama, usando-as como chave para recuperar sua contagem
#O mesmo pode ser feito com os unigramas

cond_prob = {}
for (w1, w2) in bigrams_mk:


    #calcule a probabilidade condicional aqui e armazene em prob
    prob = freq_bigram[(w1, w2)] / freq_unigram[w1]
    cond_prob[(w1, w2)] = prob

for (w1, w2), p in list(cond_prob.items())[:5]:
    print(f"P({w2} | {w1}) = {p:.4f}")

# Para verificar uma probabilidade, utilize as duas palavras como uma chave única no dicionário
print(bigrams_mk[:5])
cond_prob[('vindo', 'da')]

"""3. Utilize a função abaixo para gerar algumas sentenças com seu modelo bigrama. A função já está pronta, nada deve ser feito, apenas invocá-la passando seu dicionário (variável cond_prob) de probabilidades condicionais."""

import random

def gerar_sentenca_shannon_bigram(cond_prob):
    sentenca = []
    w1 = '<s>'

    while True:
        # Obter todas as palavras w2 possíveis dado w1
        candidatos = [(w2, prob) for (prev, w2), prob in cond_prob.items() if prev == w1]

        if not candidatos:
            break  # não há continuidade possível

        # Separar palavras e probabilidades
        palavras, probs = zip(*candidatos)

        # Amostrar uma palavra com base nas probabilidades
        w2 = random.choices(palavras, weights=probs, k=1)[0]

        if w2 == '</s>':
            break

        sentenca.append(w2)
        w1 = w2

    return ' '.join(sentenca)

#Gere e exiba a primeira sentença
sent1 = gerar_sentenca_shannon_bigram(cond_prob)
print(sent1)

#Gere e exiba a segunda sentença
sent2 = gerar_sentenca_shannon_bigram(cond_prob)
print(sent2)

#Gere e exiba a terceira sentença
sent3 = gerar_sentenca_shannon_bigram(cond_prob)
print(sent3)

"""4. Calcule a Perplexidade de uma das sentenças geradas."""

#Utilize uma das sentenças geradas e aplique o método split() para quebrá-la por espaços
#Gere os bigramas para a lista de tokens obtidas com o split()
#Percorra cada bigrama, recupere a probabilidade desse bigrama que está no dicionário cond_prob
#Multiplique todas as probabilidades e armazene o resultado na variável sentence_probabilities

sentence_tokens = sent3.split()
sentence_bigrams = list(nltk.bigrams(sentence_tokens))

sentence_probabilities = 1
for w1, w2 in sentence_bigrams:
    prob = cond_prob[(w1, w2)]
    sentence_probabilities *= prob


#Veja a perplexidade resultante
sentence_perplexity = 1 / (sentence_probabilities ** (1 / len(sentence_tokens)))
print(f"Perplexidade da sentença: {sentence_perplexity:.4f}")

"""## **Perguntas**

**1. As sentenças geradas com o algoritmo de shannon, fizeram sentido ou apresentam partes com sentido? Justifique**

`As sentenças geradas pelo algoritmo de Shannon com o modelo bigrama apresentam partes "entendiveis", mas raramente formam frases coerentes como um todo. Normalmente não tem sentido.`

# Para CASA
Tente adaptar o modelo bigrama para trigrama e compare os resultados. Essa etapa não deve ser enviada como resultado do lab. Dica: agora o início e final de cada sentença deve agora considerar dois `<s>` e `</s>`
"""