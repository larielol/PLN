# -*- coding: utf-8 -*-
"""Lab4_LucasAriel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifyr1lJNmJqbgAjaq_a9fB1P8y6TfXzk

# Dados

*   **Aluno**: Lucas Ariel Alves de Carvalho
*   **Matrícula**: 121210801
"""



"""# Representação Vetorial de Textos em PLN

A linguagem humana é rica, ambígua e altamente contextual. No entanto, para que computadores consigam processar, comparar ou aprender com textos, é necessário traduzi-los para uma linguagem que as máquinas compreendam: **números**.

É aí que entram os **modelos vetoriais de representação textual**, que transformam palavras, frases ou documentos em **vetores numéricos**. Essa etapa é chamada de **vetorização**.

Neste laboratório, você vai explorar a evolução das representações vetoriais utilizadas no Processamento de Linguagem Natural (PLN), entendendo como passamos de abordagens simples, como **One-Hot Encoding**, para técnicas mais expressivas como **TF-IDF**, além de aplicar **métricas de similaridade** e algoritmos como o **KNN** para classificar textos.

Vamos colocar a mão na massa e ver na prática como representar textos como vetores e extrair sentido dessa nova forma de ver a linguagem!

---

## Imports
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

"""## Funções"""

import numpy as np

def similaridade_cosseno(v1, v2):
    v1 = np.array(v1)
    v2 = np.array(v2)
    numerador = np.dot(v1, v2)
    denominador = np.linalg.norm(v1) * np.linalg.norm(v2)
    return numerador / denominador if denominador != 0 else 0.0

"""## Exemplo com DataSet Simples

Antes de começarmos a vetorização, precisamos de um conjunto de textos para trabalhar. Para isso, vamos utilizar um **mini-corpus com 15 frases curtas**, que simulam situações do cotidiano envolvendo animais, ambientes e clima.

Esse conjunto foi pensado para conter:
- Palavras repetidas e variações sutis (como "gato" e "gata", "chuva" e "molhado"),
- Frases com temas comuns (sofá, quintal, jardim),
- E expressões que nos permitem testar **similaridade**, **frequência** e **contexto**.

A ideia é que esse corpus nos ajude a enxergar, de forma prática, como diferentes técnicas de vetorização representam os textos e como elas capturam (ou não) relações entre palavras e frases.
"""

corpus = [
    "o gato dorme no sofá, com o outro gato",
    "o cachorro late no quintal",
    "a gata correu pelo jardim, com a outra gata",
    "o sofá é confortável",
    "meu cachorro gosta de correr",
    "gatos e cachorros são animais domésticos",
    "o quintal e as árvores está molhado por causa da chuva",
    "choveu muito no final de semana",
    "meu gato gosta de dormir o dia todo",
    "animais gostam de brincar no jardim",
    "o dia está ensolarado e quente",
    "a chuva deixou tudo molhado",
    "meus animais dormem juntos no sofá",
    "o jardim tem flores e árvores e as árvores são frutíferas",
    "o cachorro de Maria subiu no sofá"
]

"""### **Vetorização de dados textuais**

Nesta seção, você deve executar três tipos diferentes de vetorização: o ***One-hot Encoding***, o ***Term Frequency* (TF)** e o ***Term Frequency-Inverse Document Frequency* (TF-IDF)**. Após descrever as características e o funcionamento dessas vetorizações, aplique-as aos documentos da variável **`response`** do conjunto de dados.

#### One Hot Encoding

A técnica de **One-Hot Encoding** é uma das formas mais simples de representar palavras como vetores. Nela, cada palavra do vocabulário é representada por um vetor binário com a mesma dimensão do vocabulário — e apenas uma posição é "1", indicando a presença da palavra.

Abaixo, aplicamos One-Hot Encoding para cada **palavra única** do nosso corpus.
"""

# Inicializando o vetorizador com o binary = true para que as colunas geradas possuam um valor binário e não um valor de contagem
vectorizer_one_hot = CountVectorizer(binary=True)

# Vetorizando o dataset
one_hot_response = vectorizer_one_hot.fit_transform(corpus)

# Gerando um dataframe com o dataset vetorizado
one_hot_df = pd.DataFrame(one_hot_response.toarray(), columns=vectorizer_one_hot.get_feature_names_out())

# Plotando o dataset vetorizado
one_hot_df

"""# Agora é sua vez

Utilizando um DataSet de comentários do IMDB, vamos transformar cada comentário em um vetor e calcular similaridades com métricas diferentes
"""

url = 'https://raw.githubusercontent.com/Karthik-Bhaskar/Sentiment-Analysis/refs/heads/master/processed.csv'
df = pd.read_csv(url)
df = df.head(1000)
df.drop(columns=['Unnamed: 0'], inplace=True)
print('Quantidade de comentários', df.shape[0])
df.head()

"""## 1. Para cada texto do dataset do IMDB, transforme-o em sua versão vetorizada binária."""

#É possível vetorizar o informações do dataframe passando seu dataframe['nome_coluna'] como parâmetro para seu vetorizador
#Você pode utilizar o vetorizador presente no exemplo acima de nome e salvar os vetores na variável one_hot_response
#Recupere cada palavra resultante da vetorização. Dica: use a função get_feature_names_out() do seu vetorizador
vectorizer_one_hot = CountVectorizer(binary=True)
one_hot_response = vectorizer_one_hot.fit_transform(df['review'])
words = vectorizer_one_hot.get_feature_names_out()

print(words[:10])

#Transforme seus vetores em um dataframe para trabalhar com essa estrutura de dados. Use os vetores da célula anterior e as palavras obtidas para criar o dataframe
#Você pode fazer isso chamando a classe pd.DataFrame(vetores.toarray(), columns=['nome de cada coluna'])
one_hot_df = pd.DataFrame(one_hot_response.toarray(), columns=words)

one_hot_df.head()

# Recupere o primeiro documento para a variável d1
# Você pode fazer isso, recuperando o elemento índice 0 no dataframe usando loc.
d1 = one_hot_df.loc[0]

# Imprima quais palavras fazem parte do vetor de d1
# Para imprimir as palavras você pode percorrer cada posição desse vetor. Se ele for 1, então você pode manter sua palavra usando o atributo index do d1
d1_words = [word for word, val in zip(words, d1) if val == 1]

print(d1_words[:10])

# Encontre o id do documento mais similar ao documento d1 percorrendo o restante do dataframe e retorne seu índice
# Utilize o produto escalar e a similaridade do cosseno da função já disponível para achar os documentos similares por essas duas métricas
# Calcule cada similaridade separadamente e verifique se o documento retornado é igual

#Variáveis para armazenar o id e a similaridade do documento mais similar conforme a similaridade do cosseno
sim_cos = -1
id_sc = -1

#Variáveis para armazenar o id e a similaridade do documento mais similar conforme o produto escalar
prod_esc = -1
id_pe = -1

for idx, row in one_hot_df.iloc[1:].iterrows():

  curren_pe = np.dot(d1, row)
  current_sc = similaridade_cosseno(d1, row)

  #Calcule o produto escalar e salve o documento de maior similaridade
  #Você pode usar a função np.dot() passando seus vetores para calcular o produto escalar
  if curren_pe > prod_esc:
    prod_esc = curren_pe
    id_pe = idx


  #Calcule a Similaridade do cosseno e salve o documento de maior similaridade
  #Você pode usar a função de similaridade do cosseno presente no início deste notebook passando seus vetores
  if current_sc > sim_cos:
    sim_cos = current_sc
    id_sc = idx


print(f"Produto escalar: {prod_esc}")
print("Id do documento:", id_pe)
print()
print(f"Similaridade do cosseno: {sim_cos:.4f}")
print("Id do documento:", id_sc)

# Mostre as 10 primeiras palavras presentes do documento que possui maior similaridade conforme o produto escalar
# Você pode recuperar esse vetor usando o índice salvo em id_pe
d_sim_pe = one_hot_df.iloc[id_pe]
d_sim_pe_words = [word for word, val in zip(words, d_sim_pe) if val == 1]

print(d_sim_pe_words[:10])

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior similaridade do cosseno
# Você pode recuperar esse vetor usando o índice salvo em id_sc
d_sim_cos = one_hot_df.iloc[id_sc]
d_sim_cos_words = [word for word, val in zip(words, d_sim_cos) if val == 1]

print(d_sim_cos_words[:10])

#Implemente o cálculo da similaridade de Jaccard
#Lembre que a similaridade é calculada considerando |A intersect B|/|A union B|
#Você pode utilizar os recursos da classe set() do python


def similaridade_jaccard(v1: set, v2: set):
  intersection = len(v1.intersection(v2))
  union = len(v1.union(v2))
  return intersection / union

# Calcule a similaridade de Jaccard entre os dois documentos mais similares conforme o produto escalar
# Dica: você pode utilizar a classe set() do python para transformar listas de palavras em conjuntos e aplicar suas operações
# Você pode usar as variáveis d_sim_pe_words e d1_words para verificar as palavras em cada documento
jaccard = similaridade_jaccard(set(d1_words), set(d_sim_pe_words))


print(f"Similaridade de Jaccard: {jaccard:.4f}")

# Calcule a similaridade de Jaccard entre os dois documentos retornados anteriormente em relação à d1
# Dica: você pode utilizar a classe set() do python para transformar listas de palavras em conjuntos e aplicar suas operações
# Você pode usar as variáveis d_sim_cos_words e d1_words para verificar as palavras em cada documento
jaccard = similaridade_jaccard(set(d1_words), set(d_sim_cos_words))


print(f"Similaridade de Jaccard: {jaccard:.4f}")

"""## 2. Agora vamos avançar para uma abordagem que permite representar **textos inteiros** como vetores: o modelo **Bag of Words (BoW)**.

A ideia do BoW é simples, mas poderosa:  
Cada documento (ou frase) é representado por um vetor que **conta quantas vezes cada palavra do vocabulário aparece** naquele texto.


Refaça os passos da etapa 1, agora utilizando a contagem de cada palavra!
"""

#Gere um novo vetorizador e armazene na variável vectorizer_bow
#Você pode fazer isso utilizando a classe CountVectorizer() sem nenhum parâmetro
vectorizer_bow = CountVectorizer()

#É possível vetorizar o dataframe passando df['nome_coluna'] como parâmetro para seu vetorizador
#Armazene a vetorização na variável bow_response
#Recupere cada palavra resultante da vetorização. Dica: use a função get_feature_names_out() do seu vetorizador
bow_response = vectorizer_bow.fit_transform(df['review'])
bow_words = vectorizer_bow.get_feature_names_out()

print(bow_words[:10])

#Transforme seus vetores em um dataframe para trabalhar com essa estrutura de dados, use os vetores da célula anterior e as palavras obtidas
#Você pode fazer isso chamando a classe pd.DataFrame(vetores, columns=['nome de cada coluna'])
bow_df = pd.DataFrame(bow_response.toarray(), columns=bow_words)



bow_df.head()

# Recupere o primeiro documento e armazene na variável bow_d1
# Você pode fazer isso recuperando o elemento índice 0 no dataframe usando a função loc.
bow_d1 = bow_df.loc[0]

# Imprima quais palavras fazem parte de bow_d1
# Para cada posição desse vetor, se ele for diferente de 0, então você pode manter essa palavra usando a variável index do bow_d1
bow_d1_words = [word for word, val in zip(bow_words, bow_d1) if val != 0]


print(bow_d1_words[:10])

# Encontre os ids dos documentos mais similares ao documento d1 percorrendo o restante do dataframe e retorne seus índices
# Utilize o produto escalar e a similaridade do cosseno da função já disponível para achar os documentos similares por essas duas métricas
# Calcule cada similaridade separadamente e verifique se o documento retornado é igual
# Use a implementação da seção anterior

#Variáveis para armazenar o id do documento mais similar conforme a similaridade do cosseno
sim_cos = -1
id_sc = -1

#Variáveis para armazenar o id do documento mais similar conforme o produto escalar
prod_esc = -1
id_pe = -1

for idx, row in bow_df.iloc[1:].iterrows():

  curren_pe = np.dot(bow_d1, row)
  current_sc = similaridade_cosseno(bow_d1, row)

  #Calcule o produto escalar e salve o documento de maior similaridade
  #Você pode usar a função np.dot() passando seus vetores para calcular o produto escalar
  if curren_pe > prod_esc:
    prod_esc = curren_pe
    id_pe = idx

  #Calcule a Similaridade do cosseno e salve o documento de maior similaridade
  #Você pode usar a função de similaridade do cosseno presente no início deste notebook passando seus vetores
  if current_sc > sim_cos:
    sim_cos = current_sc
    id_sc = idx


print(f"Produto escalar: {prod_esc}")
print("Id do documento:", id_pe)
print()
print(f"Similaridade do cosseno: {sim_cos:.4f}")
print("Id do documento:", id_sc)

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior produto escalar
# Você pode recuperar esse vetor usando o índice salvo em id_pe
d_sim_pe = bow_df.iloc[id_pe]
d_sim_pe_words = [word for word, val in zip(bow_words, d_sim_pe) if val != 0]


print(d_sim_pe_words[:10])

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior similaridade do cosseno
# Você pode recuperar esse vetor usando o índice salvo em id_sc
d_sim_cos = bow_df.iloc[id_sc]
d_sim_cos_words = [word for word, val in zip(bow_words, d_sim_cos) if val != 0]


print(d_sim_cos_words[:10])

"""## 3. 📊 TF-IDF (Term Frequency - Inverse Document Frequency)

Até agora, usamos a contagem para representar o peso de uma palavra em um documento.

Mas pense bem: palavras como "o", "e", "no", "de" aparecem o tempo todo em quase todos os textos. Mesmo que sejam frequentes, **elas não carregam muito significado específico**.

É aí que entra o **TF-IDF**: uma técnica que ajusta o TF penalizando palavras que são comuns em muitos documentos e destacando aquelas que são mais **específicas** de um documento.

##### 🧠 Como funciona o TF-IDF?

- **TF**: frequência relativa da palavra no documento.
- **IDF**: mede o quão rara (ou informativa) a palavra é em todo o corpus.
- O TF-IDF final é o produto entre esses dois valores.

O resultado é uma representação vetorial mais equilibrada, que valoriza as palavras **relevantes**, não apenas as **frequentes**.

Agora vamos aplicar isso ao nosso corpus!
"""

#Crie o vetorizador dessa etapa. Você pode fazer isso utilizando a classe TfidfVectorizer() sem parâmetro.
#Crie seu vetorizador em uma variável chamada tfidf_vectorizer
tfidf_vectorizer = TfidfVectorizer()

#É possível vetorizar o dataframe passando df['nome_coluna'] como parâmetro para seu vetorizador
#Armazene a vetorização na variável tfidf_response
#Recupere cada palavra resultante da vetorização. Dica: use a função get_feature_names_out() do seu vetorizador
tfidf_response = tfidf_vectorizer.fit_transform(df['review'])
tfidf_words = tfidf_vectorizer.get_feature_names_out()


tfidf_words[:10]

#Transforme seus vetores em um dataframe para trabalhar com essa estrutura de dados, use os vetores da célula anterior e as palavras obtidas
#Você pode fazer isso chamando a classe pd.DataFrame(vetores, columns=['nome de cada coluna'])
tfidf_df = pd.DataFrame(tfidf_response.toarray(), columns=tfidf_words)

tfidf_df.head()

# Recupere o primeiro documento e armazene na variável bow_d1
# Você pode fazer isso, recuperando o elemento índice 0 no dataframe.
tfidf_d1 = tfidf_df.loc[0]

# Imprima quais palavras fazem parte de seu vetor
# Para cada posição desse vetor, se ele for diferente de 0, então você pode manter sua palavra usando a variável index do tfidf_d1
tfidf_d1_words = [word for word, val in zip(tfidf_words, tfidf_d1) if val != 0]

print(tfidf_d1_words[:10])

# Encontre os ids dos documentos mais similares ao documento d1 percorrendo o restante do dataframe e retorne seus índices
# Utilize o produto escalar e a similaridade do cosseno da função já disponível para achar os documentos similares por essas duas métricas
# Calcule cada similaridade separadamente e verifique se o documento retornado é igual
# Use a implementação da seção anterior

#Variáveis para armazenar o id do documento mais similar conforme a similaridade do cosseno
sim_cos = -1
id_sc = -1

#Variáveis para armazenar o id do documento mais similar conforme o produto escalar
prod_esc = -1
id_pe = -1

for idx, row in tfidf_df.iloc[1:].iterrows():
  current_pe = np.dot(tfidf_d1, row)
  current_sc = similaridade_cosseno(tfidf_d1, row)

  #Calcule o produto escalar e salve o documento de maior similaridade
  #Você pode usar a função np.dot() passando seus vetores para calcular o produto escalar
  if current_pe > prod_esc:
    prod_esc = current_pe
    id_pe = idx

  #Calcule a Similaridade do cosseno e salve o documento de maior similaridade
  #Você pode usar a função de similaridade do cosseno presente no início deste notebook passando seus vetores
  if current_sc > sim_cos:
    sim_cos = current_sc
    id_sc = idx

print(f"Produto escalar: {prod_esc}")
print("Id do documento:", id_pe)
print()
print(f"Similaridade do cosseno: {sim_cos:.4f}")
print("Id do documento:", id_sc)

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior produto escalar
# Você pode recuperar esse vetor usando o índice salvo em id_pe
d_sim_pe = tfidf_df.iloc[id_pe]
d_sim_pe_words = [word for word, val in zip(tfidf_words, d_sim_pe) if val != 0]


d_sim_pe_words[:10]

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior similaridade do cosseno
# Você pode recuperar esse vetor usando o índice salvo em id_sc
d_sim_cos = tfidf_df.iloc[id_sc]
d_sim_cos_words = [word for word, val in zip(tfidf_words, d_sim_cos) if val != 0]


d_sim_cos_words[:10]

"""#### **Perguntas**

1. **Ao se utilizar estratégias diferentes de vetorização, o resultado do documento mais similar se manteve igual? Justifique (considere a similaride do cosseno)**

Não, os resultados variaram. Pelo que dá pra perceber, o TF-IDF dá mais peso a palavras não muito comuns, enquanto o One-Hot e BoW consideram apenas a presença ou contagem.

2. **Ao se utilizar métricas de similaridade diferentes o resultado do documento mais similar se manteve igual? Justifique (considere qualquer forma de vetorização)**  
Não, o produto escalar e a similaridade do cosseno retornaram documentos diferentes principalmeente na parte 1, no TF IDF a o resultado/retorno deu semelhante. O produto escalar muda de acordo tamanho dos vetores e o cosseno é pela direção.

## KNN
O **k-NN (k-Nearest Neighbors)** é um algoritmo de classificação baseado em **proximidade**. Em PLN, usamos representações vetoriais dos textos para comparar semelhanças entre documentos.

A lógica do algoritmo é simples:

> Dado um novo texto, o k-NN procura os **k textos mais próximos** no conjunto de treinamento e decide a **classe mais comum** entre esses vizinhos.

A "proximidade" entre os textos vetorizados é determinada por uma **métrica de distância**, como:

- 🧮 **Distância Euclidiana** (métrica padrão)
- 📏 **Distância Manhattan** (soma das diferenças absolutas)
- 🧊 **Distância Chebyshev** (maior diferença em uma dimensão)

Essa abordagem nos permite:

- Classificar textos com base em exemplos anteriores
- Explorar como diferentes métricas de distância afetam os resultados
- Analisar se a **classificação faz sentido com base nas palavras presentes**

Neste experimento, vamos utilizar k-NN para prever o sentimento (positivo ou negativo) dos comentários vetorizados anteriormente.
"""

# Defina aqui os rótulos, busque no dataset qual coluna carrega o sentimento do comentário
# Para acessar a coluna do dataframe basta utilizar dataframe['nome da coluna']
rotulos = df['sentiment']

print(rotulos)

# Defina aqui qual modelo vetorizado será utilizado, podendo ser o bow ou o td-idf que foi utilizado na etapa anterior
# Utilize a variável que já armazena os dados vetorizados e salve na variável textos
textos = tfidf_response

# Separe aqui os textos na parte de treino e na parte de teste, deixando 20% dos dados para teste. Para isso utilize a função train_test_split()
# A função train_test_split() espera receber os textos, rótulos, test_size para separação em treino e teste e random_state. Use random_state=42 para evitar aleatoriedade
# Essa função retorna 4 informações: X_treino, X_teste, Y_treino, Y_teste

X_train, X_test, y_train, y_test = train_test_split(textos, rotulos, test_size=0.2, random_state=42)

# Trecho já pronto, não é necessário mudá-lo
metricas = ['euclidean', 'manhattan', 'chebyshev']

# Defina aqui o valor da quantidade de vizinhos proximos
# Sinta-se à vontade para experimentar e buscar os melhores valores de K
k = 3

#Não é necessário alterar esse trecho
for metrica in metricas:
    print(f"\n🔍 Distância: {metrica.upper()}")

    #Treinando o modelo
    knn = KNeighborsClassifier(n_neighbors=k, metric=metrica)
    knn.fit(X_train.toarray(), y_train)

    # Escolhendo um exemplo de teste
    X_exemplo = X_test[0]
    y_exemplo = y_test.iloc[0]
    texto_exemplo = df['review'][y_test.index[0]]

    # Classificando
    pred = knn.predict([X_exemplo.toarray()[0]])

    print(f"📌 Texto: {texto_exemplo}")
    print(f"✅ Classe real: {y_exemplo}")
    print(f"📈 Classe prevista: {pred[0]}")

    # Listando os K vizinhos mais próximos
    dist, idxs = knn.kneighbors([X_exemplo.toarray()[0]])
    print("👥 Documentos mais próximos:")
    for i, idx in enumerate(idxs[0]):
        viz_texto = df['review'].iloc[y_train.index[idx]]
        viz_label = y_train.iloc[idx]
        print(f"{i+1}. \"{viz_texto}\" → classe: {viz_label}")

"""# Perguntas

Responda as duas perguntas seguintes considerando k = 3

**1 . Ao se utilizar métricas de distância diferentes, a classificação do documento é igual ao seu rótulo original? Se não, quais métricas resultam em classificação diferente?**

Resposta: Apenas a distância Euclidiana está correta. Manhattan e Chebyshev estão incorreto.

**2 . Algum documento se repete como mais similar independemente da métrica? Se sim? Quais palavras ele possui em comum com o documento original?**

Resposta: Sim, o documento "unpretentious horror film..." aparece como similar em duas métricas, ele compartilha palavras como "film", "see", "much" com o documento original.

Responda a pergunta a seguir considerando k maior ou igual a 10

**3 . Se aumentarmos k, a classificação do documento continua igual ao seu rótulo original independemente da métrica? Se não, qual métrica faz com que a classificação seja incorreta?**

Resposta: A classificação pode mudar dependendo da métrica. A distância euclidiana tende a manter a classificação correta, enquanto as outras podem variar mais.

**4 . Alguma métrica de distância apresentou um resultado de classificação consistente independentemente da vetorização e do tamanho de K? Se sim, qual?**

Resposta: A distância euclidiana mostrou maior consistência na classificação correta, independente do tamanho de k e da vetorização utilizada.
"""