# -*- coding: utf-8 -*-
"""Lab4_LucasAriel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifyr1lJNmJqbgAjaq_a9fB1P8y6TfXzk

# Dados

*   **Aluno**: Lucas Ariel Alves de Carvalho
*   **Matr√≠cula**: 121210801
"""



"""# Representa√ß√£o Vetorial de Textos em PLN

A linguagem humana √© rica, amb√≠gua e altamente contextual. No entanto, para que computadores consigam processar, comparar ou aprender com textos, √© necess√°rio traduzi-los para uma linguagem que as m√°quinas compreendam: **n√∫meros**.

√â a√≠ que entram os **modelos vetoriais de representa√ß√£o textual**, que transformam palavras, frases ou documentos em **vetores num√©ricos**. Essa etapa √© chamada de **vetoriza√ß√£o**.

Neste laborat√≥rio, voc√™ vai explorar a evolu√ß√£o das representa√ß√µes vetoriais utilizadas no Processamento de Linguagem Natural (PLN), entendendo como passamos de abordagens simples, como **One-Hot Encoding**, para t√©cnicas mais expressivas como **TF-IDF**, al√©m de aplicar **m√©tricas de similaridade** e algoritmos como o **KNN** para classificar textos.

Vamos colocar a m√£o na massa e ver na pr√°tica como representar textos como vetores e extrair sentido dessa nova forma de ver a linguagem!

---

## Imports
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

"""## Fun√ß√µes"""

import numpy as np

def similaridade_cosseno(v1, v2):
    v1 = np.array(v1)
    v2 = np.array(v2)
    numerador = np.dot(v1, v2)
    denominador = np.linalg.norm(v1) * np.linalg.norm(v2)
    return numerador / denominador if denominador != 0 else 0.0

"""## Exemplo com DataSet Simples

Antes de come√ßarmos a vetoriza√ß√£o, precisamos de um conjunto de textos para trabalhar. Para isso, vamos utilizar um **mini-corpus com 15 frases curtas**, que simulam situa√ß√µes do cotidiano envolvendo animais, ambientes e clima.

Esse conjunto foi pensado para conter:
- Palavras repetidas e varia√ß√µes sutis (como "gato" e "gata", "chuva" e "molhado"),
- Frases com temas comuns (sof√°, quintal, jardim),
- E express√µes que nos permitem testar **similaridade**, **frequ√™ncia** e **contexto**.

A ideia √© que esse corpus nos ajude a enxergar, de forma pr√°tica, como diferentes t√©cnicas de vetoriza√ß√£o representam os textos e como elas capturam (ou n√£o) rela√ß√µes entre palavras e frases.
"""

corpus = [
    "o gato dorme no sof√°, com o outro gato",
    "o cachorro late no quintal",
    "a gata correu pelo jardim, com a outra gata",
    "o sof√° √© confort√°vel",
    "meu cachorro gosta de correr",
    "gatos e cachorros s√£o animais dom√©sticos",
    "o quintal e as √°rvores est√° molhado por causa da chuva",
    "choveu muito no final de semana",
    "meu gato gosta de dormir o dia todo",
    "animais gostam de brincar no jardim",
    "o dia est√° ensolarado e quente",
    "a chuva deixou tudo molhado",
    "meus animais dormem juntos no sof√°",
    "o jardim tem flores e √°rvores e as √°rvores s√£o frut√≠feras",
    "o cachorro de Maria subiu no sof√°"
]

"""### **Vetoriza√ß√£o de dados textuais**

Nesta se√ß√£o, voc√™ deve executar tr√™s tipos diferentes de vetoriza√ß√£o: o ***One-hot Encoding***, o ***Term Frequency* (TF)** e o ***Term Frequency-Inverse Document Frequency* (TF-IDF)**. Ap√≥s descrever as caracter√≠sticas e o funcionamento dessas vetoriza√ß√µes, aplique-as aos documentos da vari√°vel **`response`** do conjunto de dados.

#### One Hot Encoding

A t√©cnica de **One-Hot Encoding** √© uma das formas mais simples de representar palavras como vetores. Nela, cada palavra do vocabul√°rio √© representada por um vetor bin√°rio com a mesma dimens√£o do vocabul√°rio ‚Äî e apenas uma posi√ß√£o √© "1", indicando a presen√ßa da palavra.

Abaixo, aplicamos One-Hot Encoding para cada **palavra √∫nica** do nosso corpus.
"""

# Inicializando o vetorizador com o binary = true para que as colunas geradas possuam um valor bin√°rio e n√£o um valor de contagem
vectorizer_one_hot = CountVectorizer(binary=True)

# Vetorizando o dataset
one_hot_response = vectorizer_one_hot.fit_transform(corpus)

# Gerando um dataframe com o dataset vetorizado
one_hot_df = pd.DataFrame(one_hot_response.toarray(), columns=vectorizer_one_hot.get_feature_names_out())

# Plotando o dataset vetorizado
one_hot_df

"""# Agora √© sua vez

Utilizando um DataSet de coment√°rios do IMDB, vamos transformar cada coment√°rio em um vetor e calcular similaridades com m√©tricas diferentes
"""

url = 'https://raw.githubusercontent.com/Karthik-Bhaskar/Sentiment-Analysis/refs/heads/master/processed.csv'
df = pd.read_csv(url)
df = df.head(1000)
df.drop(columns=['Unnamed: 0'], inplace=True)
print('Quantidade de coment√°rios', df.shape[0])
df.head()

"""## 1. Para cada texto do dataset do IMDB, transforme-o em sua vers√£o vetorizada bin√°ria."""

#√â poss√≠vel vetorizar o informa√ß√µes do dataframe passando seu dataframe['nome_coluna'] como par√¢metro para seu vetorizador
#Voc√™ pode utilizar o vetorizador presente no exemplo acima de nome e salvar os vetores na vari√°vel one_hot_response
#Recupere cada palavra resultante da vetoriza√ß√£o. Dica: use a fun√ß√£o get_feature_names_out() do seu vetorizador
vectorizer_one_hot = CountVectorizer(binary=True)
one_hot_response = vectorizer_one_hot.fit_transform(df['review'])
words = vectorizer_one_hot.get_feature_names_out()

print(words[:10])

#Transforme seus vetores em um dataframe para trabalhar com essa estrutura de dados. Use os vetores da c√©lula anterior e as palavras obtidas para criar o dataframe
#Voc√™ pode fazer isso chamando a classe pd.DataFrame(vetores.toarray(), columns=['nome de cada coluna'])
one_hot_df = pd.DataFrame(one_hot_response.toarray(), columns=words)

one_hot_df.head()

# Recupere o primeiro documento para a vari√°vel d1
# Voc√™ pode fazer isso, recuperando o elemento √≠ndice 0 no dataframe usando loc.
d1 = one_hot_df.loc[0]

# Imprima quais palavras fazem parte do vetor de d1
# Para imprimir as palavras voc√™ pode percorrer cada posi√ß√£o desse vetor. Se ele for 1, ent√£o voc√™ pode manter sua palavra usando o atributo index do d1
d1_words = [word for word, val in zip(words, d1) if val == 1]

print(d1_words[:10])

# Encontre o id do documento mais similar ao documento d1 percorrendo o restante do dataframe e retorne seu √≠ndice
# Utilize o produto escalar e a similaridade do cosseno da fun√ß√£o j√° dispon√≠vel para achar os documentos similares por essas duas m√©tricas
# Calcule cada similaridade separadamente e verifique se o documento retornado √© igual

#Vari√°veis para armazenar o id e a similaridade do documento mais similar conforme a similaridade do cosseno
sim_cos = -1
id_sc = -1

#Vari√°veis para armazenar o id e a similaridade do documento mais similar conforme o produto escalar
prod_esc = -1
id_pe = -1

for idx, row in one_hot_df.iloc[1:].iterrows():

  curren_pe = np.dot(d1, row)
  current_sc = similaridade_cosseno(d1, row)

  #Calcule o produto escalar e salve o documento de maior similaridade
  #Voc√™ pode usar a fun√ß√£o np.dot() passando seus vetores para calcular o produto escalar
  if curren_pe > prod_esc:
    prod_esc = curren_pe
    id_pe = idx


  #Calcule a Similaridade do cosseno e salve o documento de maior similaridade
  #Voc√™ pode usar a fun√ß√£o de similaridade do cosseno presente no in√≠cio deste notebook passando seus vetores
  if current_sc > sim_cos:
    sim_cos = current_sc
    id_sc = idx


print(f"Produto escalar: {prod_esc}")
print("Id do documento:", id_pe)
print()
print(f"Similaridade do cosseno: {sim_cos:.4f}")
print("Id do documento:", id_sc)

# Mostre as 10 primeiras palavras presentes do documento que possui maior similaridade conforme o produto escalar
# Voc√™ pode recuperar esse vetor usando o √≠ndice salvo em id_pe
d_sim_pe = one_hot_df.iloc[id_pe]
d_sim_pe_words = [word for word, val in zip(words, d_sim_pe) if val == 1]

print(d_sim_pe_words[:10])

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior similaridade do cosseno
# Voc√™ pode recuperar esse vetor usando o √≠ndice salvo em id_sc
d_sim_cos = one_hot_df.iloc[id_sc]
d_sim_cos_words = [word for word, val in zip(words, d_sim_cos) if val == 1]

print(d_sim_cos_words[:10])

#Implemente o c√°lculo da similaridade de Jaccard
#Lembre que a similaridade √© calculada considerando |A intersect B|/|A union B|
#Voc√™ pode utilizar os recursos da classe set() do python


def similaridade_jaccard(v1: set, v2: set):
  intersection = len(v1.intersection(v2))
  union = len(v1.union(v2))
  return intersection / union

# Calcule a similaridade de Jaccard entre os dois documentos mais similares conforme o produto escalar
# Dica: voc√™ pode utilizar a classe set() do python para transformar listas de palavras em conjuntos e aplicar suas opera√ß√µes
# Voc√™ pode usar as vari√°veis d_sim_pe_words e d1_words para verificar as palavras em cada documento
jaccard = similaridade_jaccard(set(d1_words), set(d_sim_pe_words))


print(f"Similaridade de Jaccard: {jaccard:.4f}")

# Calcule a similaridade de Jaccard entre os dois documentos retornados anteriormente em rela√ß√£o √† d1
# Dica: voc√™ pode utilizar a classe set() do python para transformar listas de palavras em conjuntos e aplicar suas opera√ß√µes
# Voc√™ pode usar as vari√°veis d_sim_cos_words e d1_words para verificar as palavras em cada documento
jaccard = similaridade_jaccard(set(d1_words), set(d_sim_cos_words))


print(f"Similaridade de Jaccard: {jaccard:.4f}")

"""## 2. Agora vamos avan√ßar para uma abordagem que permite representar **textos inteiros** como vetores: o modelo **Bag of Words (BoW)**.

A ideia do BoW √© simples, mas poderosa:  
Cada documento (ou frase) √© representado por um vetor que **conta quantas vezes cada palavra do vocabul√°rio aparece** naquele texto.


Refa√ßa os passos da etapa 1, agora utilizando a contagem de cada palavra!
"""

#Gere um novo vetorizador e armazene na vari√°vel vectorizer_bow
#Voc√™ pode fazer isso utilizando a classe CountVectorizer() sem nenhum par√¢metro
vectorizer_bow = CountVectorizer()

#√â poss√≠vel vetorizar o dataframe passando df['nome_coluna'] como par√¢metro para seu vetorizador
#Armazene a vetoriza√ß√£o na vari√°vel bow_response
#Recupere cada palavra resultante da vetoriza√ß√£o. Dica: use a fun√ß√£o get_feature_names_out() do seu vetorizador
bow_response = vectorizer_bow.fit_transform(df['review'])
bow_words = vectorizer_bow.get_feature_names_out()

print(bow_words[:10])

#Transforme seus vetores em um dataframe para trabalhar com essa estrutura de dados, use os vetores da c√©lula anterior e as palavras obtidas
#Voc√™ pode fazer isso chamando a classe pd.DataFrame(vetores, columns=['nome de cada coluna'])
bow_df = pd.DataFrame(bow_response.toarray(), columns=bow_words)



bow_df.head()

# Recupere o primeiro documento e armazene na vari√°vel bow_d1
# Voc√™ pode fazer isso recuperando o elemento √≠ndice 0 no dataframe usando a fun√ß√£o loc.
bow_d1 = bow_df.loc[0]

# Imprima quais palavras fazem parte de bow_d1
# Para cada posi√ß√£o desse vetor, se ele for diferente de 0, ent√£o voc√™ pode manter essa palavra usando a vari√°vel index do bow_d1
bow_d1_words = [word for word, val in zip(bow_words, bow_d1) if val != 0]


print(bow_d1_words[:10])

# Encontre os ids dos documentos mais similares ao documento d1 percorrendo o restante do dataframe e retorne seus √≠ndices
# Utilize o produto escalar e a similaridade do cosseno da fun√ß√£o j√° dispon√≠vel para achar os documentos similares por essas duas m√©tricas
# Calcule cada similaridade separadamente e verifique se o documento retornado √© igual
# Use a implementa√ß√£o da se√ß√£o anterior

#Vari√°veis para armazenar o id do documento mais similar conforme a similaridade do cosseno
sim_cos = -1
id_sc = -1

#Vari√°veis para armazenar o id do documento mais similar conforme o produto escalar
prod_esc = -1
id_pe = -1

for idx, row in bow_df.iloc[1:].iterrows():

  curren_pe = np.dot(bow_d1, row)
  current_sc = similaridade_cosseno(bow_d1, row)

  #Calcule o produto escalar e salve o documento de maior similaridade
  #Voc√™ pode usar a fun√ß√£o np.dot() passando seus vetores para calcular o produto escalar
  if curren_pe > prod_esc:
    prod_esc = curren_pe
    id_pe = idx

  #Calcule a Similaridade do cosseno e salve o documento de maior similaridade
  #Voc√™ pode usar a fun√ß√£o de similaridade do cosseno presente no in√≠cio deste notebook passando seus vetores
  if current_sc > sim_cos:
    sim_cos = current_sc
    id_sc = idx


print(f"Produto escalar: {prod_esc}")
print("Id do documento:", id_pe)
print()
print(f"Similaridade do cosseno: {sim_cos:.4f}")
print("Id do documento:", id_sc)

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior produto escalar
# Voc√™ pode recuperar esse vetor usando o √≠ndice salvo em id_pe
d_sim_pe = bow_df.iloc[id_pe]
d_sim_pe_words = [word for word, val in zip(bow_words, d_sim_pe) if val != 0]


print(d_sim_pe_words[:10])

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior similaridade do cosseno
# Voc√™ pode recuperar esse vetor usando o √≠ndice salvo em id_sc
d_sim_cos = bow_df.iloc[id_sc]
d_sim_cos_words = [word for word, val in zip(bow_words, d_sim_cos) if val != 0]


print(d_sim_cos_words[:10])

"""## 3. üìä TF-IDF (Term Frequency - Inverse Document Frequency)

At√© agora, usamos a contagem para representar o peso de uma palavra em um documento.

Mas pense bem: palavras como "o", "e", "no", "de" aparecem o tempo todo em quase todos os textos. Mesmo que sejam frequentes, **elas n√£o carregam muito significado espec√≠fico**.

√â a√≠ que entra o **TF-IDF**: uma t√©cnica que ajusta o TF penalizando palavras que s√£o comuns em muitos documentos e destacando aquelas que s√£o mais **espec√≠ficas** de um documento.

##### üß† Como funciona o TF-IDF?

- **TF**: frequ√™ncia relativa da palavra no documento.
- **IDF**: mede o qu√£o rara (ou informativa) a palavra √© em todo o corpus.
- O TF-IDF final √© o produto entre esses dois valores.

O resultado √© uma representa√ß√£o vetorial mais equilibrada, que valoriza as palavras **relevantes**, n√£o apenas as **frequentes**.

Agora vamos aplicar isso ao nosso corpus!
"""

#Crie o vetorizador dessa etapa. Voc√™ pode fazer isso utilizando a classe TfidfVectorizer() sem par√¢metro.
#Crie seu vetorizador em uma vari√°vel chamada tfidf_vectorizer
tfidf_vectorizer = TfidfVectorizer()

#√â poss√≠vel vetorizar o dataframe passando df['nome_coluna'] como par√¢metro para seu vetorizador
#Armazene a vetoriza√ß√£o na vari√°vel tfidf_response
#Recupere cada palavra resultante da vetoriza√ß√£o. Dica: use a fun√ß√£o get_feature_names_out() do seu vetorizador
tfidf_response = tfidf_vectorizer.fit_transform(df['review'])
tfidf_words = tfidf_vectorizer.get_feature_names_out()


tfidf_words[:10]

#Transforme seus vetores em um dataframe para trabalhar com essa estrutura de dados, use os vetores da c√©lula anterior e as palavras obtidas
#Voc√™ pode fazer isso chamando a classe pd.DataFrame(vetores, columns=['nome de cada coluna'])
tfidf_df = pd.DataFrame(tfidf_response.toarray(), columns=tfidf_words)

tfidf_df.head()

# Recupere o primeiro documento e armazene na vari√°vel bow_d1
# Voc√™ pode fazer isso, recuperando o elemento √≠ndice 0 no dataframe.
tfidf_d1 = tfidf_df.loc[0]

# Imprima quais palavras fazem parte de seu vetor
# Para cada posi√ß√£o desse vetor, se ele for diferente de 0, ent√£o voc√™ pode manter sua palavra usando a vari√°vel index do tfidf_d1
tfidf_d1_words = [word for word, val in zip(tfidf_words, tfidf_d1) if val != 0]

print(tfidf_d1_words[:10])

# Encontre os ids dos documentos mais similares ao documento d1 percorrendo o restante do dataframe e retorne seus √≠ndices
# Utilize o produto escalar e a similaridade do cosseno da fun√ß√£o j√° dispon√≠vel para achar os documentos similares por essas duas m√©tricas
# Calcule cada similaridade separadamente e verifique se o documento retornado √© igual
# Use a implementa√ß√£o da se√ß√£o anterior

#Vari√°veis para armazenar o id do documento mais similar conforme a similaridade do cosseno
sim_cos = -1
id_sc = -1

#Vari√°veis para armazenar o id do documento mais similar conforme o produto escalar
prod_esc = -1
id_pe = -1

for idx, row in tfidf_df.iloc[1:].iterrows():
  current_pe = np.dot(tfidf_d1, row)
  current_sc = similaridade_cosseno(tfidf_d1, row)

  #Calcule o produto escalar e salve o documento de maior similaridade
  #Voc√™ pode usar a fun√ß√£o np.dot() passando seus vetores para calcular o produto escalar
  if current_pe > prod_esc:
    prod_esc = current_pe
    id_pe = idx

  #Calcule a Similaridade do cosseno e salve o documento de maior similaridade
  #Voc√™ pode usar a fun√ß√£o de similaridade do cosseno presente no in√≠cio deste notebook passando seus vetores
  if current_sc > sim_cos:
    sim_cos = current_sc
    id_sc = idx

print(f"Produto escalar: {prod_esc}")
print("Id do documento:", id_pe)
print()
print(f"Similaridade do cosseno: {sim_cos:.4f}")
print("Id do documento:", id_sc)

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior produto escalar
# Voc√™ pode recuperar esse vetor usando o √≠ndice salvo em id_pe
d_sim_pe = tfidf_df.iloc[id_pe]
d_sim_pe_words = [word for word, val in zip(tfidf_words, d_sim_pe) if val != 0]


d_sim_pe_words[:10]

# Mostre as 10 primeiras palavras presentes do documento retornado por possuir maior similaridade do cosseno
# Voc√™ pode recuperar esse vetor usando o √≠ndice salvo em id_sc
d_sim_cos = tfidf_df.iloc[id_sc]
d_sim_cos_words = [word for word, val in zip(tfidf_words, d_sim_cos) if val != 0]


d_sim_cos_words[:10]

"""#### **Perguntas**

1. **Ao se utilizar estrat√©gias diferentes de vetoriza√ß√£o, o resultado do documento mais similar se manteve igual? Justifique (considere a similaride do cosseno)**

N√£o, os resultados variaram. Pelo que d√° pra perceber, o TF-IDF d√° mais peso a palavras n√£o muito comuns, enquanto o One-Hot e BoW consideram apenas a presen√ßa ou contagem.

2. **Ao se utilizar m√©tricas de similaridade diferentes o resultado do documento mais similar se manteve igual? Justifique (considere qualquer forma de vetoriza√ß√£o)**  
N√£o, o produto escalar e a similaridade do cosseno retornaram documentos diferentes principalmeente na parte 1, no TF IDF a o resultado/retorno deu semelhante. O produto escalar muda de acordo tamanho dos vetores e o cosseno √© pela dire√ß√£o.

## KNN
O **k-NN (k-Nearest Neighbors)** √© um algoritmo de classifica√ß√£o baseado em **proximidade**. Em PLN, usamos representa√ß√µes vetoriais dos textos para comparar semelhan√ßas entre documentos.

A l√≥gica do algoritmo √© simples:

> Dado um novo texto, o k-NN procura os **k textos mais pr√≥ximos** no conjunto de treinamento e decide a **classe mais comum** entre esses vizinhos.

A "proximidade" entre os textos vetorizados √© determinada por uma **m√©trica de dist√¢ncia**, como:

- üßÆ **Dist√¢ncia Euclidiana** (m√©trica padr√£o)
- üìè **Dist√¢ncia Manhattan** (soma das diferen√ßas absolutas)
- üßä **Dist√¢ncia Chebyshev** (maior diferen√ßa em uma dimens√£o)

Essa abordagem nos permite:

- Classificar textos com base em exemplos anteriores
- Explorar como diferentes m√©tricas de dist√¢ncia afetam os resultados
- Analisar se a **classifica√ß√£o faz sentido com base nas palavras presentes**

Neste experimento, vamos utilizar k-NN para prever o sentimento (positivo ou negativo) dos coment√°rios vetorizados anteriormente.
"""

# Defina aqui os r√≥tulos, busque no dataset qual coluna carrega o sentimento do coment√°rio
# Para acessar a coluna do dataframe basta utilizar dataframe['nome da coluna']
rotulos = df['sentiment']

print(rotulos)

# Defina aqui qual modelo vetorizado ser√° utilizado, podendo ser o bow ou o td-idf que foi utilizado na etapa anterior
# Utilize a vari√°vel que j√° armazena os dados vetorizados e salve na vari√°vel textos
textos = tfidf_response

# Separe aqui os textos na parte de treino e na parte de teste, deixando 20% dos dados para teste. Para isso utilize a fun√ß√£o train_test_split()
# A fun√ß√£o train_test_split() espera receber os textos, r√≥tulos, test_size para separa√ß√£o em treino e teste e random_state. Use random_state=42 para evitar aleatoriedade
# Essa fun√ß√£o retorna 4 informa√ß√µes: X_treino, X_teste, Y_treino, Y_teste

X_train, X_test, y_train, y_test = train_test_split(textos, rotulos, test_size=0.2, random_state=42)

# Trecho j√° pronto, n√£o √© necess√°rio mud√°-lo
metricas = ['euclidean', 'manhattan', 'chebyshev']

# Defina aqui o valor da quantidade de vizinhos proximos
# Sinta-se √† vontade para experimentar e buscar os melhores valores de K
k = 3

#N√£o √© necess√°rio alterar esse trecho
for metrica in metricas:
    print(f"\nüîç Dist√¢ncia: {metrica.upper()}")

    #Treinando o modelo
    knn = KNeighborsClassifier(n_neighbors=k, metric=metrica)
    knn.fit(X_train.toarray(), y_train)

    # Escolhendo um exemplo de teste
    X_exemplo = X_test[0]
    y_exemplo = y_test.iloc[0]
    texto_exemplo = df['review'][y_test.index[0]]

    # Classificando
    pred = knn.predict([X_exemplo.toarray()[0]])

    print(f"üìå Texto: {texto_exemplo}")
    print(f"‚úÖ Classe real: {y_exemplo}")
    print(f"üìà Classe prevista: {pred[0]}")

    # Listando os K vizinhos mais pr√≥ximos
    dist, idxs = knn.kneighbors([X_exemplo.toarray()[0]])
    print("üë• Documentos mais pr√≥ximos:")
    for i, idx in enumerate(idxs[0]):
        viz_texto = df['review'].iloc[y_train.index[idx]]
        viz_label = y_train.iloc[idx]
        print(f"{i+1}. \"{viz_texto}\" ‚Üí classe: {viz_label}")

"""# Perguntas

Responda as duas perguntas seguintes considerando k = 3

**1 . Ao se utilizar m√©tricas de dist√¢ncia diferentes, a classifica√ß√£o do documento √© igual ao seu r√≥tulo original? Se n√£o, quais m√©tricas resultam em classifica√ß√£o diferente?**

Resposta: Apenas a dist√¢ncia Euclidiana est√° correta. Manhattan e Chebyshev est√£o incorreto.

**2 . Algum documento se repete como mais similar independemente da m√©trica? Se sim? Quais palavras ele possui em comum com o documento original?**

Resposta: Sim, o documento "unpretentious horror film..." aparece como similar em duas m√©tricas, ele compartilha palavras como "film", "see", "much" com o documento original.

Responda a pergunta a seguir considerando k maior ou igual a 10

**3 . Se aumentarmos k, a classifica√ß√£o do documento continua igual ao seu r√≥tulo original independemente da m√©trica? Se n√£o, qual m√©trica faz com que a classifica√ß√£o seja incorreta?**

Resposta: A classifica√ß√£o pode mudar dependendo da m√©trica. A dist√¢ncia euclidiana tende a manter a classifica√ß√£o correta, enquanto as outras podem variar mais.

**4 . Alguma m√©trica de dist√¢ncia apresentou um resultado de classifica√ß√£o consistente independentemente da vetoriza√ß√£o e do tamanho de K? Se sim, qual?**

Resposta: A dist√¢ncia euclidiana mostrou maior consist√™ncia na classifica√ß√£o correta, independente do tamanho de k e da vetoriza√ß√£o utilizada.
"""