# -*- coding: utf-8 -*-
"""LAB06_LucasAriel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJxcbeSks2XFrqjIS7a_IMMgMcrf5Qkj

# Dependências
"""

!pip install gensim

"""# Agrupamento de textos com K-Means

Neste notebook, usaremos o algoritmo k-means, um algoritmo simples e popular de agrupamento não-supervisionado, para agrupar letras de músicas de samba.

O objetivo do K-means é simples: agrupar pontos de dados semelhantes e descobrir padrões subjacentes. Para atingir esse objetivo, o K-means precisa de um número K de centróides em um conjunto de dados. Um centróide refere-se a um cluster, que é uma coleção de pontos de dados agregados devido a certas semelhanças entre si. As médias/means no K-means referem-se à média dos dados; isto é, encontrar o centróide. E o algoritmo é dito não supervisionado porque não temos conhecimento prévio sobre os grupos ou classes de nosso conjunto de dados, ou seja, encontraremos os grupos subjacentes em nosso conjunto de dados!

Abaixo podemos visualizar o algoritmo. Os centróides verdes correspondem aos pontos de dados mais próximos de cada um e formam clusters, então cada centróide se move para o centro de cada respectivo grupo e combina novamente os pontos de dados mais próximos entre si.

![alt text](https://github.com/lucas-de-sa/national-anthems-clustering/blob/master/Images/kmeans.gif?raw=true)

**Passos:**

__1.__ Explorar nossa coleção de músicas(corpus) <br>
__2.__ Aplicar engenharia de dados no conjunto de dados para obter o melhor desempenho do algoritmo K-means <br>
__3.__ Execute o algoritmo várias vezes, cada vez testando com um número diferente de clusters <br>
__4.__ Use diferentes métricas para visualizar nossos resultados e encontrar o melhor número de clusters (*ou seja, por que um total de X clusters é melhor do que um total de Y clusters?*) <br>
__5.__ Análise de cluster

**Métricas utilizadas para determinar o melhor número de K Cluters:**
- [Método do cotovelo](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)
- [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

## Importando bibliotecas
"""

import pandas as pd
import numpy as np
import altair as alt

from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import re
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans

from wordcloud import WordCloud
from sklearn.manifold import TSNE

from sklearn.feature_extraction.text import CountVectorizer
from gensim.models import Word2Vec

"""# Algumas Funções úteis"""

def counting_words(df, predict, column):
  labels = np.unique(predict)
  couting = []
  for i in labels:
    mask = df[column] == i
    next_to_cluster = df[mask]
    a = next_to_cluster['clean_lyrics'].str.split()
    freq_word = []
    for i in a:
      freq_word = freq_word + i
    couting.append(nltk.FreqDist(freq_word))

  return couting

def generate_word_clouds(centroids):
    wordcloud = WordCloud(max_font_size=100, background_color = 'white')
    for i in range(0, len(centroids)):
        centroid_dict = centroids[i]
        wordcloud.generate_from_frequencies(centroid_dict)

        plt.figure()
        plt.title(f'Cluster {i}')
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.show()

#Cálculo da PPMI
def compute_ppmi_smoothed(Xc, alpha=0.75):
    total = np.sum(Xc)
    sum_over_rows = np.sum(Xc, axis=1, keepdims=True)  # P(w)
    sum_over_cols = np.sum(Xc, axis=0, keepdims=True)  # P(c)

    # aplica suavização exponencial
    sum_over_rows_alpha = np.power(sum_over_rows, alpha)
    sum_over_cols_alpha = np.power(sum_over_cols, alpha)

    expected = sum_over_rows_alpha @ sum_over_cols_alpha / total
    with np.errstate(divide='ignore', invalid='ignore'):
        pmi = np.log2((Xc * total) / expected)
        pmi[np.isinf(pmi)] = 0.0
        pmi[np.isnan(pmi)] = 0.0
        pmi = np.maximum(pmi, 0)  # PPMI
    return pmi

"""## Analisando os dados"""

# Carregando um dataset de letras de músicas
letras = pd.read_csv('https://github.com/nazareno/palavras-nas-letras/raw/master/letras-ptbr-sample.csv')
#Subamostrando devido restrições de memória do colab
letras = letras.sample(1000, random_state=42)
letras.sample(10)

#Informações das colunas do nosso dataframe
print(letras.info())

#Artistas mais presentes nesse sample
letras['Artist'].value_counts()[0:20]

"""## Pré processamento do texto"""

sw = stopwords.words('portuguese')
personalizadas = ['pra', 'mim', 'vou', 'vai', 'vem', 'tão','tudo','quer']
sw = sw + personalizadas
clean_lyrics = []

for lyric in letras['Lyric']:
    # Remove caracteres especiais e dígitos
    lyric = re.sub(r"(\d|\W|_)+|\w*\d\w*", " ", lyric)

    # Divide em palavras
    words = lyric.lower().split()

    # Remove stopwords e palavras curtas ou com números
    words = [w for w in words if w not in sw and len(w) > 2 and not any(c.isdigit() for c in w)]

    # Junta de volta
    cleaned = ' '.join(words)
    clean_lyrics.append(cleaned)

clean_lyrics[:10]

#Adicionando as letras preprocessadas no dataframe original
letras['clean_lyrics'] = clean_lyrics

#Liberando memória
del clean_lyrics

letras

"""## Vetores TF-IDF
Nesta etapa, as músicas são vetorizadas aplicando TFIDF. Ainda não está sendo empregado a estratégia de buscar os embeddings das palavras e transformar as sentenças em um vetor com base nos vetores de suas palavras.
"""

tfv = TfidfVectorizer(
        min_df = 10,
        max_df = 0.5,
        max_features = None,
        stop_words = sw
  )

vec_text = tfv.fit_transform(letras['clean_lyrics'])

words = tfv.get_feature_names_out()

len(words)

words[1:10]

"""## Agrupando com o K-Means

Para simplificar o trabalho, vamos escolher **4 grupos**. Ou seja, vamos tentar identificar quais músicas são mais parecidas entre si, considerando apenas 4 grupos distintos.

PS: existem métodos específicos para tentar definir um melhor k com base nos próprios dados.
"""

k = 4

#Nesta etapa, um objeto da classe kmeans é instanciado e executamos o algoritmo considerando cada música vetorizada por tfidf
kmeans = MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20)
kmeans.fit(vec_text)
labels_tfidf = kmeans.predict(vec_text)

#Criando um novo dataframe
letras['tfidf_group'] = labels_tfidf

letras.sample(10)

#Verificando a quantidade de letras atribuídas a cada grupo
letras['tfidf_group'].value_counts()

#Listando os cantores presentes em cada grupo e a quantidade de músicas dele que faz parte do grupo
for g in range(0, k):
  print('\n-----\nGRUPO {}:'.format(g))
  print(letras.query('tfidf_group ==  {}'.format(g))['Artist'].value_counts()[0:10])
  print('-----')

# Exibindo as palavras mais comuns e cada grupo
generate_word_clouds(counting_words(letras,labels_tfidf, 'tfidf_group'))

#Exibindo alguns regristo de um certo grupo
#Escolha um grupo e analise os artistas, letras e gêneros

pd.options.display.max_colwidth = 100
letras.query('tfidf_group == 2')[['SName', 'Lyric', 'Artist', 'Genre']].sample(10)

"""## PPMI (Positive Pontual Multiwise Individual)

Agora vamos trabalhar com um método diferente de vetorização. Vamos calcular o PPMI de cada palavra e depois transformar as letras utilizando os embeddings de cada palavra em cada letra.

1. Utilize um vetorizador para contar cada palavra (bag-of-words) da coluna clean_lyrics do dataframe. Salve o resultado da vetorização na variável X
"""

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(letras['clean_lyrics'])

#Essa etapa está pronta, não é necessário mudá-la. Aqui estamos gerando a matriz de co-ocorrência das palavras
Xc = (X.T @ X).toarray()  # coocorrência entre palavras

"""2. Invoque a função compute_ppmi_smoothed para calcular o PPMi de cada palavra. Você pode fazer isso passando a variável Xc como argumento do método. Salve o resultado na variável ppmi_matrix"""

ppmi_matrix = compute_ppmi_smoothed(Xc)

"""3. Crie um dataframe chamado ppmi_df. Para essa criação você precisa da variável ppmi_matrix e do nome de cada coluna. Os nomes podem ser obtidos utilizando o vetorizador criado anteriormente e invocando o método get_features_names_out(). A criação pode ser feita utilizando o método pd.Dataframe(seu_dados, index=suas_palavras, columns=suas_palavras)"""

ppmi_df = pd.DataFrame(ppmi_matrix, index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())

ppmi_df.head()

#Esse trecho não precisa ser alterado. Estamos apenas liberando espaço na memória
del ppmi_matrix

"""4. Complete o método abaixo para que ele seja capaz de transformar cada letra das músicas em sua forma vetorizada utilizando os embeddigns das palavras do dataframe ppmi_df."""

#Esse método está parcialmente implementado
def lyric_embeddings_from_ppmi(df, text_column, ppmi_df):

    #Essa variável irá salvar os embeddings de cada letra
    embeddings = []
    words_in_vocab = set(ppmi_df.index)

    for lyric in df[text_column]:

        #1. Para cada letra do dataframe, primeiro separe as palavras pelo espaço

        #2. Para cada palavra que foi separada por espaço, busque no dataframe ppmi_df seu embedding e salve-a em um vetor auxiliar
        # Você pode fazer buscar um embedding de uma palavra usando o método .loc[sua_palavra].values do dataframe ppmi_df
        # Lembre que, para cada embedding, você precisa adicioná-lo ao vetor temporário.


        #3. Calcule a média de todos os embeddings salvos no vetor temporário da etapa anterior
        # Você pode fazer isso utilizado a o método np.mean(seu_vetor_de_vetores, axis=0) e salvando o resultado na variável lyric_vector


        #4. Salvando o embedding de cada letra, ou seja, a versão vetorizada da letra.
        embeddings.append(lyric_vector)

    # Criando um novo DataFrame com as representações e retornando-a
    embedding_df = pd.DataFrame(embeddings, index=df.index)
    return embedding_df

# Essa etapa já está pronta, não é necessário mudá-la. Aqui, passamos nosso dataframe de letras e recebemos um dataframe de embeddings
ppmi_embeddings_df = lyric_embeddings_from_ppmi(letras, 'clean_lyrics', ppmi_df)
ppmi_embeddings_df.head()

"""5. Agora, vamos executar o Kmeans como nosso dataframe ppmi_embeddings_df. Você pode ver o exemplo de uso do KMeans na Seção Inicial deste notebook.

- Use k = 4
- Crie um novo objeto do tipo MiniBatchKMeans
- Salve as classificações na variável labels_pmi
- Faça uma nova coluna no dataframe letras (letras['pmi_group']) e atribua labels_pmi
"""

k = 4
MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20)
kmeans.fit(ppmi_embeddings_df)
labels_pmi = kmeans.predict(ppmi_embeddings_df)

letras['pmi_group'] = labels_pmi

letras.sample(10)

"""6. A partir daqui, não é necessário nenhuma implementação. Apenas execute as células para realizar análises sobre os resultados."""

#Verificando a quantidade de letras por grupo.
letras['pmi_group'].value_counts()

#Listando os cantores presentes em cada grupo e a quantidade de músicas dele que faz parte do grupo
for g in range(0, k):
  print('\n-----\nGRUPO {}:'.format(g))
  print(letras.query('pmi_group ==  {}'.format(g))['Artist'].value_counts()[0:10])
  print('-----')

generate_word_clouds(counting_words(letras,labels_pmi, 'pmi_group'))

#Exibindo alguns regristo de um certo grupo
#Escolha um grupo e analise os artistas, letras e gêneros

pd.options.display.max_colwidth = 100
letras.query('pmi_group == 3')[['SName', 'Lyric', 'Artist', 'Genre']].sample(10)

"""## Perguntas sobre o PMI

Responda cada perguntando apresentando uma justificativa

1. Considerando os artistas de cada grupo, é possível verificar alguma conexão entre eles? (Ex: gênero, estilo de música, temas abordados)

A única conexão é pelas palavras em cada música, como"amor", "coração", "paixão" que são muito utilizadas mas em questão de gênero não tem conexão.

2. Considerando as nuvens de palavras de cada grupo, argumente se existe um grupo bem definido ou se não existe uma definição clara sobre que tipo de música (tema) cada grupo tem.

Com o PPMI, os grupos de palavras geralmente têm uma definição é mais clara porque algumas pralavras aparecem frequentemente juntas, por exemplo, "festa", "dança", "curtição" em um grupo; "saudade", "voltar", "esquecer" em outro. Isso faz com que cada grupo tenha um tema bem definido e fácil de interpretar.

3. De maneira geral, você acha que os resultados do PPMI são mais consistentes que os do TFIDF?

Sim, os resultados do PPMI são mais consistentes e úteis para entender temas. O TF-IDF se concentra em palavras que são raras, o que nem sempre cria grupos temáticos lógicos. O PPMI foca em palavras que estão estatisticamente associadas, palavras que "andam juntas" nas frases, fazendo que tenham um mesmo "tema".

# Word2Vec

Agora vamos trabalhar com o Word2Vec. Primeiramente vamos treinar os embeddings considerando todas as letras do nosso dataset. Em seguida, vamos vetorizar essas letras utilizando os embeddings das palavras de cada letra. Em seguida, vamos aplicar o KMeans e analisar os grupos.

1. Inicialmente, precisamos quebrar cada letra do nosso dataset em uma lista de palavras. Isso deve ser feito para cada letra. O resultado deve ser um vetor de vetores, em que cada subvetor é o conjunto de palavras da letra atual. Lembre-se que você deve considerar as letras que estão na coluna 'clean_lyric' do dataframe.
"""

tokenized_lyrics = [lyric.split() for lyric in letras['clean_lyrics']]

"""2. Chame a classe Word2Vec() passando para ela o vetor de letras que estão separadas por espaço. Além disso, utilize os seguintes parâmetros:

- vector_size=100,  # tamanho dos vetores
- window=5,
- min_count=2,
- workers=4,
- epochs=15,
- sg=1


- Salve o modelo na variável w2v_model
"""

w2v_model = Word2Vec(sentences=tokenized_lyrics, vector_size=100, window=5, min_count=2, workers=4, epochs=15, sg=1)

w2v_model.wv['amor']

"""3. Complete o método abaixo para que ele seja capaz de transformar cada letra das músicas em sua forma vetorizada utilizando os embeddigns das palavras que você pode obter do modelo w2v_model."""

#Este método está parcialmente completo. Ele tem algumas similaridades com o método de
def lyric_embeddings_from_w2v(df, text_column, w2v_model):

    #Aqui será armazenando o embedding final de cada letra.
    embeddings = []
    vocab = w2v_model.wv.key_to_index

    for lyric in df[text_column]:

        #1. Para cada letra do dataframe, primeiro separe as palavras pelo espaço
        words = lyric.split()


        #2. Para cada palavra que foi separada por espaço, busque no w2v_model seu embedding e salve-a em um vetor auxiliar
        # Você pode fazer buscar um embedding de uma palavra usando o método w2v_model.wv[sua_palavra]
        # Lembre que, para cada embedding, você precisa adicioná-lo ao vetor temporário.
        word_vectors = []
        for word in words:
            if word in vocab:
                word_vectors.append(w2v_model.wv[word])


        #3. Calcule a média de todos os embeddings salvos no vetor temporário da etapa anterior
        # Você pode fazer isso utilizado a o método np.mean(seu_vetor_de_vetores, axis=0) e salvando o resultado na variável lyric_vector
        lyric_vector = np.mean(word_vectors, axis=0)



        #4. Salvando o embedding de cada letra, ou seja, a versão vetorizada da letra.
        embeddings.append(lyric_vector)

    # Cria um DataFrame com os embeddings
    embedding_df = pd.DataFrame(embeddings, index=df.index)
    return embedding_df

# Esta parte está pronta, não é necessário alterá-la
w2v_embeddings_df = lyric_embeddings_from_w2v(letras, 'clean_lyrics', w2v_model)
w2v_embeddings_df.head()

"""4. Agora, vamos executar o Kmeans como nosso dataframe w2v_embeddings_df. Você pode ver o exemplo de uso do KMeans na Seção Inicial deste notebook.
- Use k = 4
- Crie um novo objeto do tipo MiniBatchKMeans()
- Salve as classificações na variável labels_w2v
- Faça uma nova coluna no dataframe letras (letras['w2v_group']) e atribua labels_w2v
"""

k = 4
MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20)
kmeans.fit(w2v_embeddings_df)
labels_w2v = kmeans.predict(w2v_embeddings_df)

letras['w2v_group'] = labels_w2v

letras.sample(10)

"""5. A partir daqui, não é necessário nenhuma implementação. Apenas execute as células para realizar análises sobre os resultados."""

#Verificando a quantidade de letras por grupo.
letras['w2v_group'].value_counts()

#Listando os cantores presentes em cada grupo e a quantidade de músicas dele que faz parte do grupo
for g in range(0, k):
  print('\n-----\nGRUPO {}:'.format(g))
  print(letras.query('w2v_group ==  {}'.format(g))['Artist'].value_counts()[0:10])
  print('-----')

generate_word_clouds(counting_words(letras,labels_w2v, 'w2v_group'))

"""## Perguntas sobre o Word2Vec
Responda cada perguntando apresentando uma justificativa

1. Considerando os artistas de cada grupo, é possível verificar alguma conexão entre eles? (Ex: gênero, estilo de música, temas abordados)

Sim, o Word2Vec agrupa as músicas com base no contexto em que as palavras são usadas. Músicas com temas similares, como "amor", "festa" ou "política", terão vetores de palavras parecidos, e a média desses vetores (que representa a música) as colocará próximas umas das outras. Mas em relação ao genero musical não é tão parecido.

2. Considerando as nuvens de palavras de cada grupo, argumente se existe um grupo bem definido ou se não existe uma definição clara sobre que tipo de música (tema) cada grupo tem.

Os grupos de palavras tendem a ter uma definição muito clara e coerente porque vai além da simples co-ocorrência e cria vetores que capturam o significado e as relações semânticas entre as palavras. Cria um conjunto de palavras que têm um significado comum. As "nuvens" de palavras de cada grupo são muito coesas e representar um tema óbvio (por exemplo, "romance", "festa", "religiosidade"), tornando a definição de cada grupo muito mais nítida.

3. De maneira geral, você acha que os resultados com o Word2Vec são mais consistentes que os dos métodos anteriores?

Sim porque consistência se refere à capacidade do método de agrupar itens de maneira semanticamente lógica.

  O TF-IDF se baseia na frequência das palavras, o que pode agrupar músicas com base em vocabulários raros, mas não necessariamente em temas.

  O PPMI melhora ao focar na co-ocorrência, mas ainda tem um limite.

  O Word2Vec, aprende a representação vetorial de cada palavra a partir de um contexto de palavras vizinhas. Isso permite que ele compreenda que palavras como "carro" e "automóvel" são sinônimos. Essa compreensão mais "profunda" faz com que os agrupamentos de músicas sejam os mais tematicamente coerentes.

A única coisa que destoa um pouco dos resultados é em relação ao gênero, pois todos não tem relação, como por exemplo com o teste em algumas palavras tem músicas de Rock, Pop, Samba, Hip-Hop...
"""